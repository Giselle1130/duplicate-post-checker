import os
import re
import time
import traceback
from datetime import datetime, date as date_cls
from shutil import which

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait

try:
    from webdriver_manager.chrome import ChromeDriverManager
    USE_WDM = True
except Exception:
    USE_WDM = False

try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


# =========================
# ëŒ€ìƒ ê²Œì‹œíŒ ê³ ì •
# =========================
CLUB_ID = 28866679
MENU_ID = 178

# âœ… ì•ˆì •ì ì¸ "í´ë˜ì‹ ëª©ë¡"ìœ¼ë¡œ ì§„ì…
BASE_LIST_URL = (
    "https://cafe.naver.com/ArticleList.nhn"
    f"?search.clubid={CLUB_ID}"
    f"&search.menuid={MENU_ID}"
    "&search.boardtype=L"
)

# ê¸€ë²ˆí˜¸ ì¶”ì¶œ: href / onclick / ë°ì´í„° ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ ì§€ì›
ARTICLEID_RE = re.compile(
    r"(?:[?&]articleid=(\d+))|(?:/articles/(\d+))|(?:articleid[=:]\s*['\"]?(\d+))",
    re.IGNORECASE,
)

# ëª©ë¡ì—ì„œ ê¸€ ë§í¬ í›„ë³´ (href/onclick/data-articleid í¬í•¨)
LINK_CSS = (
    "a[href*='articleid='], a[href*='/articles/'], "
    "a[onclick*='articleid'], a[onclick*='/articles/'], "
    "a[data-articleid]"
)


# =========================
# ìœ í‹¸
# =========================
def clean(x: str) -> str:
    return (x or "").replace("\u200b", "").strip()


def kst_today() -> date_cls:
    if KST is None:
        return datetime.now().date()
    return datetime.now(KST).date()


def is_time_token(s: str) -> bool:
    return re.fullmatch(r"\d{1,2}:\d{2}", (s or "").strip()) is not None


def extract_time_token(text: str) -> str:
    m = re.search(r"\b(\d{1,2}:\d{2})\b", clean(text))
    return m.group(1) if m else ""


def extract_date_token_any(text: str):
    """
    âœ… í•µì‹¬ ìˆ˜ì •:
    - 2025.12.16 / 2025.12.16. (ì—°ë„ í¬í•¨)
    - 12.16 / 12.16. (ì—°ë„ ì—†ìŒ) ëª¨ë‘ ì¸ì‹
    """
    t = clean(text)

    m1 = re.search(r"\b(20\d{2})\.(\d{2})\.(\d{2})\.?\b", t)
    if m1:
        y, mo, d = int(m1.group(1)), int(m1.group(2)), int(m1.group(3))
        return date_cls(y, mo, d)

    m2 = re.search(r"\b(\d{2})\.(\d{2})\.?\b", t)
    if m2:
        mo, d = int(m2.group(1)), int(m2.group(2))
        return ("MD", mo, d)

    return None


def build_page_url(page: int) -> str:
    return f"{BASE_LIST_URL}&search.page={page}"


def canonical_article_link(article_id: str) -> str:
    # âœ… ê°™ì€ ê¸€ì´ë©´ ë§í¬ê°€ ì„ì—¬ë„ ë™ì¼í•˜ê²Œ ì €ì¥(ì¤‘ë³µ ì œê±° ì•ˆì •)
    return f"https://cafe.naver.com/ca-fe/cafes/{CLUB_ID}/articles/{article_id}"


# =========================
# ì œëª© ì •ê·œí™”/í† í°
# =========================
STOPWORDS = {
    "steam", "kakao", "paragon", "pubg",
    "í´ëœ", "í´ëœì›", "ëª¨ì§‘", "í™˜ì˜", "ê°€ì…",
    "ë””ìŠ¤ì½”ë“œ", "discord", "ì„œë²„",
    "ì´ˆë³´", "ì‹ ìƒ", "ì¹œëª©", "ê²½ìŸ", "ì§ì¥ì¸",
    "ì¼ë°˜", "ë­í¬", "ë­ê²œ", "ìŠ¤ì¿¼ë“œ", "ë“€ì˜¤", "ì†”ë¡œ",
    "ë‚´ì „", "ììœ ", "ì´ë²¤íŠ¸", "ì•ˆë‚´", "ê³µì§€",
}


def normalize_title(raw: str) -> str:
    t = clean(raw)

    # ë ëŒ“ê¸€ìˆ˜ ì œê±°
    t = re.sub(r"\s*\[\s*\d+\s*\]\s*$", "", t)
    t = re.sub(r"\s*\(\s*\d+\s*\)\s*$", "", t)

    # [Steam] ê°™ì€ íƒœê·¸ ì œê±°
    t = re.sub(r"\[[^\]]{1,30}\]", " ", t)

    # LV / ë‚˜ì´/ë²”ìœ„ íŒ¨í„´ ì œê±°
    t = re.sub(r"\bLv\.?\s*\d+\b", " ", t, flags=re.IGNORECASE)
    t = re.sub(r"\b\d{1,2}\s*~\s*\d{1,2}\b", " ", t)
    t = re.sub(r"\b\d{1,2}\s*ì„¸\b", " ", t)

    # url ì œê±°
    t = re.sub(r"https?://\S+", " ", t)

    # ì´ëª¨ì§€/ê¸°í˜¸ ì œê±° (í•œ/ì˜/ìˆ«ì/ê³µë°±ë§Œ ìœ ì§€)
    t = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", t)

    # ìˆ«ì ë‹¨ë… ì œê±°
    t = re.sub(r"\b\d+\b", " ", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t


def tokenize(text: str):
    t = normalize_title(text)
    toks = re.findall(r"[a-z]+|[ê°€-í£]+", t)
    toks = [x for x in toks if len(x) >= 2]
    toks = [x for x in toks if x not in STOPWORDS]
    return toks


# =========================
# Chrome/Chromium ë°”ì´ë„ˆë¦¬ íƒì§€
# =========================
def _find_chrome_binary():
    # 1) ì‚¬ìš©ìê°€ í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•œ ê²½ìš°
    env = os.environ.get("CHROME_BIN") or os.environ.get("GOOGLE_CHROME_BIN")
    if env and os.path.exists(env):
        return env

    # 2) PATH íƒìƒ‰
    for name in ["google-chrome", "google-chrome-stable", "chromium", "chromium-browser"]:
        p = which(name)
        if p:
            return p

    # 3) í”í•œ ì„¤ì¹˜ ê²½ë¡œ
    candidates = [
        "/usr/bin/google-chrome",
        "/usr/bin/google-chrome-stable",
        "/usr/bin/chromium",
        "/usr/bin/chromium-browser",
        "/snap/bin/chromium",
    ]
    for p in candidates:
        if os.path.exists(p):
            return p
    return None


# =========================
# Selenium
# =========================
def make_driver(headless: bool = True) -> webdriver.Chrome:
    opts = Options()
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1400,900")

    # âœ… ì†ë„/ì•ˆì •
    opts.page_load_strategy = "eager"

    if headless:
        opts.add_argument("--headless=new")
        # ì¼ë¶€ ë¦¬ëˆ…ìŠ¤/WSLì—ì„œ ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ìˆìŒ
        opts.add_argument("--remote-debugging-port=0")

    # âœ… ì´ë¯¸ì§€ ì°¨ë‹¨(ì†ë„â†‘)
    opts.add_experimental_option("prefs", {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
    })

    # âœ… ìë™í™” íƒì§€ ì™„í™”(ê°€ëŠ¥í•œ ë²”ìœ„)
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)

    # âœ… í¬ë¡¬/í¬ë¡œë¯¸ì›€ ê²½ë¡œ ì§€ì •(ì—†ìœ¼ë©´ ë“œë¼ì´ë²„ê°€ ë°”ë¡œ ì£½ëŠ” í™˜ê²½ì´ ë§ìŒ)
    chrome_bin = _find_chrome_binary()
    if chrome_bin:
        opts.binary_location = chrome_bin

    try:
        if USE_WDM:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=opts)
        else:
            driver = webdriver.Chrome(options=opts)
    except Exception as e:
        msg = str(e)
        hint = [
            "í¬ë¡¬/í¬ë¡œë¯¸ì›€ ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì¤˜.",
            "Ubuntu/WSLì´ë©´: sudo apt update && sudo apt install -y google-chrome-stable (ë˜ëŠ” chromium-browser)",
            "ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ë°ë„ ì‹¤íŒ¨í•˜ë©´ CHROME_BIN í™˜ê²½ë³€ìˆ˜ë¡œ í¬ë¡¬ ê²½ë¡œë¥¼ ì§€ì •í•´ì¤˜. ì˜ˆ) export CHROME_BIN=/usr/bin/google-chrome",
        ]
        raise RuntimeError("ChromeDriver ì‹¤í–‰ ì‹¤íŒ¨\n\nì›ì¸:\n" + msg + "\n\ní•´ê²°:\n- " + "\n- ".join(hint))

    driver.implicitly_wait(0.5)

    # navigator.webdriver ìˆ¨ê¹€(ê°€ëŠ¥í•œ ë²”ìœ„)
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"}
        )
    except Exception:
        pass

    return driver


def switch_to_cafe_main_iframe(driver) -> bool:
    # ë„¤ì´ë²„ ì¹´í˜(í´ë˜ì‹)ëŠ” cafe_main iframeì— ëª©ë¡ì´ ëœ¨ëŠ” ê²½ìš°ê°€ ë§ìŒ
    try:
        driver.switch_to.default_content()
        if driver.find_elements(By.ID, "cafe_main"):
            driver.switch_to.frame("cafe_main")
            return True
    except Exception:
        pass
    return False


def wait_list_loaded(driver):
    """
    âœ… í•µì‹¬: 'ê¸€ ë§í¬'ê°€ ì‹¤ì œë¡œ ìƒê¸¸ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼.
    - iframe ì•ˆ/ë°– ëª¨ë‘ ì²´í¬
    - link íŒ¨í„´ ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ í—ˆìš©
    """
    wait = WebDriverWait(driver, 25)

    def has_links_in_current_doc(d):
        return len(d.find_elements(By.CSS_SELECTOR, LINK_CSS)) > 0

    # 1) iframe ë¨¼ì €
    if switch_to_cafe_main_iframe(driver):
        try:
            wait.until(has_links_in_current_doc)
            return
        except Exception:
            pass

    # 2) default contentì—ì„œ ë‹¤ì‹œ ì²´í¬
    try:
        driver.switch_to.default_content()
    except Exception:
        pass

    wait.until(has_links_in_current_doc)


def is_notice_row(row_text: str, row_el) -> bool:
    t = clean(row_text)
    lines = [x.strip() for x in t.split("\n") if x.strip()]
    if any(x == "ê³µì§€" for x in lines):
        return True
    try:
        if row_el is not None and len(row_el.find_elements(By.XPATH, ".//*[normalize-space()='ê³µì§€']")) > 0:
            return True
    except Exception:
        pass
    return False


def pick_row_author(row_text: str, title: str) -> str:
    t = clean(row_text)
    lines = [x.strip() for x in t.split("\n") if x.strip()]
    lines = [x for x in lines if x != title]
    lines = [x for x in lines if not is_time_token(x)]
    # ë‚ ì§œ í† í°(ì—°ë„ í¬í•¨/ë¯¸í¬í•¨) ì œê±°
    lines = [x for x in lines if not re.fullmatch(r"(20\d{2}\.\d{2}\.\d{2}\.?)|(\d{2}\.\d{2}\.?)", x)]
    bad = ["ì¡°íšŒ", "ì¢‹ì•„ìš”", "ëŒ“ê¸€", "ëŒ“ê¸€ìˆ˜"]
    lines = [x for x in lines if not any(b in x for b in bad)]
    lines = [x for x in lines if x != "ê³µì§€"]
    for x in lines:
        if 1 <= len(x) <= 30:
            return x
    return ""


def extract_article_id(el) -> str:
    """
    âœ… hrefê°€ ë¹„ì–´ìˆê±°ë‚˜ javascriptì¸ ê²½ìš°ë„ ì»¤ë²„:
    - data-articleid
    - href
    - onclick
    """
    try:
        da = clean(el.get_attribute("data-articleid"))
        if da.isdigit():
            return da
    except Exception:
        pass

    try:
        href = clean(el.get_attribute("href"))
        if href:
            m = ARTICLEID_RE.search(href)
            if m:
                return (m.group(1) or m.group(2) or m.group(3) or "").strip()
    except Exception:
        pass

    try:
        onclick = clean(el.get_attribute("onclick"))
        if onclick:
            m = ARTICLEID_RE.search(onclick)
            if m:
                return (m.group(1) or m.group(2) or m.group(3) or "").strip()
    except Exception:
        pass

    return ""


def collect_by_paging(
    target_date: date_cls,
    headless: bool,
    max_pages: int,
    stop_no_match_pages: int,
    pause: float,
):
    today = kst_today()
    is_today = (target_date == today)

    driver = make_driver(headless=headless)
    collected = {}
    no_match_pages = 0

    try:
        for page in range(1, int(max_pages) + 1):
            driver.get(build_page_url(page))

            wait_list_loaded(driver)
            time.sleep(pause)

            # âœ… table/tr ìš°ì„ , ì—†ìœ¼ë©´ lië„ ë³´ì¡°
            rows = driver.find_elements(By.CSS_SELECTOR, "tr")
            if len(rows) < 5:
                rows = driver.find_elements(By.CSS_SELECTOR, "li") + rows

            page_matches = 0

            for row in rows:
                try:
                    row_text = clean(row.text)
                    if not row_text:
                        continue
                    if is_notice_row(row_text, row):
                        continue

                    links = row.find_elements(By.CSS_SELECTOR, LINK_CSS)
                    if not links:
                        continue

                    a = links[0]

                    # ê¸€ë²ˆí˜¸ í™•ë³´(í•µì‹¬)
                    article_id = extract_article_id(a)
                    if not article_id:
                        article_id = extract_article_id(row)
                    if not article_id:
                        continue

                    # ì œëª©
                    title_raw = clean(a.text)
                    if not title_raw:
                        lines = [x.strip() for x in row_text.split("\n") if x.strip()]
                        title_raw = lines[0] if lines else ""
                    if not title_raw:
                        continue

                    hhmm = extract_time_token(row_text)
                    dtok = extract_date_token_any(row_text)

                    if is_today:
                        # ì˜¤ëŠ˜: ì‹œê°„í˜•ë§Œ ìˆ˜ì§‘
                        if not hhmm:
                            continue
                        date_raw = hhmm
                    else:
                        # ê³¼ê±°: ë‚ ì§œí˜•ë§Œ ìˆ˜ì§‘(ì—°ë„ í¬í•¨/ë¯¸í¬í•¨ ëª¨ë‘ í—ˆìš©)
                        if hhmm:
                            continue

                        ok = False
                        date_raw = ""

                        if isinstance(dtok, date_cls):
                            ok = (dtok == target_date)
                            date_raw = dtok.strftime("%Y.%m.%d")
                        elif isinstance(dtok, tuple) and dtok[0] == "MD":
                            _, mo, d = dtok
                            try:
                                d_obj = date_cls(target_date.year, mo, d)
                                ok = (d_obj == target_date)
                                date_raw = d_obj.strftime("%Y.%m.%d")
                            except Exception:
                                ok = False

                        if not ok:
                            continue

                    link = canonical_article_link(article_id)

                    collected[link] = {
                        "date": target_date.strftime("%Y-%m-%d"),
                        "date_raw": date_raw,
                        "author": pick_row_author(row_text, title_raw),
                        "title": title_raw,
                        "title_norm": normalize_title(title_raw),
                        "link": link,
                    }
                    page_matches += 1

                except Exception:
                    continue

            if page_matches > 0:
                no_match_pages = 0
            else:
                no_match_pages += 1

            # âœ… ì¡°ê¸° ì¢…ë£Œ
            if no_match_pages >= int(stop_no_match_pages):
                break

            time.sleep(pause)

    finally:
        try:
            driver.quit()
        except Exception:
            pass

    df = pd.DataFrame(list(collected.values()))
    if not df.empty:
        df = df.drop_duplicates(subset=["link"]).copy()
        df = df.sort_values(by="date_raw", ascending=False)
    return df.to_dict("records")


# =========================
# ì¤‘ë³µ/ìœ ì‚¬ (ìºì‹œë¡œ ë ‰ ì™„í™”)
# =========================
@st.cache_data(show_spinner=False)
def compute_author_dups_cached(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=["date", "author", "count"])
    a = df.groupby(["date", "author"]).size().reset_index(name="count")
    return a[a["count"] >= 2].sort_values(by="count", ascending=False)


@st.cache_data(show_spinner=False)
def compute_exact_dups_cached(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=df.columns)
    return df[df.duplicated(subset=["date", "title_norm"], keep=False)].copy()


@st.cache_data(show_spinner=False)
def compute_keyword_groups_cached(df: pd.DataFrame, min_count: int = 2):
    """
    âœ… ê°™ì€ í‚¤ì›Œë“œê°€ 2ê±´ ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ì¡ê¸°(ì‘ì„±ì ë¬´ê´€)
    """
    if df.empty:
        return pd.DataFrame(columns=["keyword", "count", "examples"])

    tokens_list = [tokenize(x) for x in df["title"].fillna("").astype(str).tolist()]

    inv = {}
    for idx, toks in enumerate(tokens_list):
        for tok in set(toks):
            inv.setdefault(tok, []).append(idx)

    rows = []
    for kw, idxs in inv.items():
        if len(idxs) >= min_count:
            ex = [df.iloc[i]["title"] for i in idxs[:3]]
            rows.append({
                "keyword": kw,
                "count": len(idxs),
                "examples": " | ".join(ex),
            })

    out = pd.DataFrame(rows)
    if out.empty:
        return out
    return out.sort_values(by=["count", "keyword"], ascending=[False, True])


@st.cache_data(show_spinner=False)
def compute_ai_similar_cached(df: pd.DataFrame, threshold: float = 0.78, max_n: int = 250) -> pd.DataFrame:
    """
    âœ… ì‘ì„±ì ì¡°ê±´ ì œê±°: ì„ íƒ ë‚ ì§œ ì „ì²´ì—ì„œ ìœ ì‚¬ë„ ë¹„êµ
    âœ… ë ‰ ë°©ì§€: max_nê°œë§Œ ë¹„êµ(ìµœì‹ ìˆœ head)
    """
    cols = ["title_a", "title_b", "similarity", "link_a", "link_b"]
    if df.empty or len(df) < 2:
        return pd.DataFrame(columns=cols)

    df2 = df.copy()
    if len(df2) > max_n:
        df2 = df2.head(max_n).copy()

    titles_raw = df2["title"].fillna("").astype(str).tolist()
    titles = df2["title_norm"].fillna("").astype(str).tolist()
    links = df2["link"].fillna("").astype(str).tolist()

    vec_w = TfidfVectorizer(analyzer="word", ngram_range=(1, 2), min_df=1)
    Xw = vec_w.fit_transform(titles)
    Mw = cosine_similarity(Xw)

    vec_c = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 5), min_df=1)
    Xc = vec_c.fit_transform(titles)
    Mc = cosine_similarity(Xc)

    M = 0.55 * Mw + 0.45 * Mc

    rows = []
    n = len(titles)
    for i in range(n):
        for j in range(i + 1, n):
            s = float(M[i, j])
            if s >= threshold:
                rows.append({
                    "title_a": titles_raw[i],
                    "title_b": titles_raw[j],
                    "similarity": round(s, 3),
                    "link_a": links[i],
                    "link_b": links[j],
                })

    out = pd.DataFrame(rows, columns=cols)
    return out.sort_values(by="similarity", ascending=False) if not out.empty else out


# =========================
# UI
# =========================
st.set_page_config(page_title="menu=178 ìˆ˜ì§‘/ì¤‘ë³µ", layout="wide")
st.title("ğŸ°â”ƒí´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ(menu=178)")

with st.expander("ì„¤ì •", expanded=True):
    c1, c2, c3, c4, c5, c6 = st.columns([1, 1, 1, 1, 1, 1])
    with c1:
        target_date = st.date_input("ë‚ ì§œ ì„ íƒ", value=kst_today())
    with c2:
        headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤", value=True)
    with c3:
        max_pages = st.number_input("ìµœëŒ€ í˜ì´ì§€", min_value=1, max_value=500, value=120, step=5)
    with c4:
        stop_no_match_pages = st.number_input("ì—°ì† 0í˜ì´ì§€ë©´ ì¢…ë£Œ", min_value=1, max_value=10, value=2, step=1)
    with c5:
        pause = st.number_input("í˜ì´ì§€ ëŒ€ê¸°(ì´ˆ)", min_value=0.05, max_value=2.00, value=0.15, step=0.05)
    with c6:
        run_ai = st.checkbox("ğŸ¤– AI ìœ ì‚¬ë„ ê³„ì‚°(ë¬´ê±°ì›€)", value=False)

c7, c8, c9 = st.columns([1, 1, 1])
with c7:
    keyword_min_count = st.number_input("í‚¤ì›Œë“œ ì¤‘ë³µ ìµœì†Œ ê±´ìˆ˜", min_value=2, max_value=20, value=2, step=1)
with c8:
    sim_threshold = st.slider("AI ìœ ì‚¬ë„ ê¸°ì¤€", 0.50, 0.99, 0.78, 0.01)
with c9:
    ai_max_n = st.number_input("AI ë¹„êµ ìµœëŒ€ ê¸€ ìˆ˜", min_value=50, max_value=800, value=250, step=50)

st.divider()

if st.button("ìˆ˜ì§‘ ì‹œì‘", use_container_width=True):
    st.session_state.posts = []
    try:
        posts = collect_by_paging(
            target_date=target_date,
            headless=headless,
            max_pages=int(max_pages),
            stop_no_match_pages=int(stop_no_match_pages),
            pause=float(pause),
        )
        st.session_state.posts = posts
        st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ")
    except Exception as e:
        st.error("ìˆ˜ì§‘ ì˜¤ë¥˜")
        st.code(str(e))
        st.code(traceback.format_exc())

df = (
    pd.DataFrame(st.session_state.posts)
    if "posts" in st.session_state and st.session_state.posts
    else pd.DataFrame(columns=["date", "date_raw", "author", "title", "title_norm", "link"])
)

author_dups = compute_author_dups_cached(df)
exact_dups = compute_exact_dups_cached(df)
keyword_groups = compute_keyword_groups_cached(df, min_count=int(keyword_min_count))

ai_similar = pd.DataFrame(columns=["title_a", "title_b", "similarity", "link_a", "link_b"])
if run_ai:
    ai_similar = compute_ai_similar_cached(df, threshold=float(sim_threshold), max_n=int(ai_max_n))

tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Œ ì›ë³¸", "ğŸš¨ ì‘ì„±ì ë™ì¼", "ğŸ§¨ ì œëª© ë™ì¼", "ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", "ğŸ¤– AI ìœ ì‚¬"])

with tab1:
    st.dataframe(df, use_container_width=True)

with tab2:
    if author_dups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")
    else:
        st.dataframe(author_dups, use_container_width=True)

with tab3:
    if exact_dups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")
    else:
        st.dataframe(exact_dups, use_container_width=True)

with tab4:
    if keyword_groups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")
    else:
        st.dataframe(keyword_groups, use_container_width=True)

with tab5:
    if not run_ai:
        st.info("AI ìœ ì‚¬ë„ëŠ” ë¬´ê±°ì›Œì„œ ê¸°ë³¸ OFFì•¼. ìœ„ ì„¤ì •ì—ì„œ ì²´í¬í•˜ë©´ ê³„ì‚°í•´.")
    if ai_similar.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")
    else:
        st.dataframe(ai_similar, use_container_width=True)
