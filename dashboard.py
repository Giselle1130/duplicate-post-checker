import re
import time
from datetime import datetime, date as date_cls, timedelta
from urllib.parse import urljoin

import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ---------------------
# KST (ë°°í¬ ì•ˆì „)
# ---------------------
try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


def kst_now() -> datetime:
    if KST:
        return datetime.now(KST)
    return datetime.utcnow() + timedelta(hours=9)


def kst_today() -> date_cls:
    return kst_now().date()


# =====================
# ê²Œì‹œíŒ ê³ ì •
# =====================
CLUB_ID = 28866679
MENU_ID = 178
BASE_URL = f"https://cafe.naver.com/f-e/cafes/{CLUB_ID}/menus/{MENU_ID}?viewType=L&page="

BASE_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Connection": "keep-alive",
}


# =====================
# âœ… ë„¤ì´ë²„ ë¡œê·¸ì¸(ì¿ í‚¤) ì ìš© í—¤ë”
# =====================
def get_headers() -> dict:
    headers = BASE_HEADERS.copy()

    # 1) secrets ìš°ì„  (Render í™˜ê²½ë³€ìˆ˜/Streamlit secretsë¡œ ë„£ì„ ìˆ˜ ìˆìŒ)
    nid_aut = st.secrets.get("NID_AUT", "") if hasattr(st, "secrets") else ""
    nid_ses = st.secrets.get("NID_SES", "") if hasattr(st, "secrets") else ""

    # 2) UI ì…ë ¥ê°’(ì„¸ì…˜) ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°
    cookie = st.session_state.get("naver_cookie", "")
    if cookie:
        headers["Cookie"] = cookie
        return headers

    # 3) secretsê°€ ìˆìœ¼ë©´ cookie êµ¬ì„±
    if nid_aut and nid_ses:
        headers["Cookie"] = f"NID_AUT={nid_aut}; NID_SES={nid_ses}"
        return headers

    # 4) ì¿ í‚¤ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë°˜í™˜ (ë°°í¬ì—ì„œëŠ” ë§‰í ìˆ˜ ìˆìŒ)
    return headers


# =====================
# ë‚ ì§œ í…ìŠ¤íŠ¸ í•´ì„ (ì˜¤ëŠ˜/ê³¼ê±° í†µì¼)
# =====================
def infer_date_from_list_text(date_text: str) -> date_cls | None:
    s = (date_text or "").strip()

    # HH:MM => ì˜¤ëŠ˜(KST)
    if re.match(r"^\d{1,2}:\d{2}$", s):
        return kst_today()

    # YYYY.MM.DD => í•´ë‹¹ ë‚ ì§œ
    m = re.match(r"^(\d{4})\.(\d{2})\.(\d{2})$", s)
    if m:
        y, mo, d = map(int, m.groups())
        try:
            return date_cls(y, mo, d)
        except Exception:
            return None

    return None


def is_target_date(date_text: str, target_date: date_cls) -> bool:
    inferred = infer_date_from_list_text(date_text)
    return inferred == target_date


# =====================
# í…ìŠ¤íŠ¸ ì •ê·œí™”
# =====================
def norm(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def normalize_title(s: str) -> str:
    s = norm(s).lower()
    s = re.sub(r"[^\wê°€-í£ ]+", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokens(s: str) -> list[str]:
    s = (s or "").lower()
    s = re.sub(r"[^0-9a-zê°€-í£ ]+", " ", s)
    return [p for p in s.split() if len(p) >= 2]


# =====================
# ëª©ë¡/ë³¸ë¬¸ ìˆ˜ì§‘
# =====================
def fetch_list_page(page: int):
    url = BASE_URL + str(page)
    res = requests.get(url, headers=get_headers(), timeout=25, allow_redirects=True)
    return url, res


def collect_article_list(target_date: date_cls, max_pages: int, debug: bool = False):
    articles = []
    debug_log = []

    for page in range(1, max_pages + 1):
        url, res = fetch_list_page(page)
        html = res.text or ""
        soup = BeautifulSoup(html, "html.parser")

        # 1) ìš°ì„  a.article
        links = soup.select("a.article")

        # 2) ì—†ìœ¼ë©´ /articles/ í¬í•¨ ë§í¬ë¡œ fallback
        if not links:
            links = [a for a in soup.select("a[href]") if "/articles/" in (a.get("href") or "")]

        if debug:
            sample_dates = [dt.get_text(strip=True) for dt in soup.select("td.td_date")[:10]]
            debug_log.append(
                {
                    "page": page,
                    "status": res.status_code,
                    "final_url": res.url,
                    "found_links": len(links),
                    "sample_date_texts": ", ".join(sample_dates) if sample_dates else "(none)",
                    "html_head": html[:400].replace("\n", " "),
                }
            )

        # âœ… 1í˜ì´ì§€ë¶€í„° ë§í¬ê°€ 0ê°œë©´: ì°¨ë‹¨/ë¹ˆ HTML ê°€ëŠ¥ì„±ì´ ë§¤ìš° í¼ â†’ ë” ëŒë ¤ë„ ì†Œìš© ì—†ìŒ
        if page == 1 and len(links) == 0:
            break

        for a in links:
            href = a.get("href") or ""
            if not href:
                continue

            title = a.get_text(strip=True) or a.get("title", "") or ""

            # date/authorëŠ” ë³´í†µ ê°™ì€ tr ì•ˆì— ìˆìŒ
            date_text = ""
            author = ""

            row = a.find_parent("tr")
            if row:
                dt = row.select_one("td.td_date")
                if dt:
                    date_text = dt.get_text(strip=True)
                au = row.select_one("td.td_name")
                if au:
                    author = au.get_text(strip=True)

            # ë‚ ì§œ ëª» ì¡ì•˜ìœ¼ë©´ ì£¼ë³€ì—ì„œ í•œë²ˆ ë”
            if not date_text:
                near = a.find_parent()
                if hasattr(near, "select_one"):
                    dt2 = near.select_one("td.td_date")
                    if dt2:
                        date_text = dt2.get_text(strip=True)

            # âœ… ë‚ ì§œ í•„í„°
            if not is_target_date(date_text, target_date):
                continue

            full_url = urljoin("https://cafe.naver.com", href)

            articles.append(
                {
                    "date": target_date.strftime("%Y-%m-%d"),
                    "date_raw": date_text,
                    "author": author,
                    "title": title,
                    "title_norm": normalize_title(title),
                    "link": full_url,
                }
            )

        time.sleep(0.25)

    return articles, debug_log


def fetch_content(url: str) -> str:
    try:
        res = requests.get(url, headers=get_headers(), timeout=25, allow_redirects=True)
        if res.status_code != 200:
            return ""

        soup = BeautifulSoup(res.text, "html.parser")

        iframe = soup.select_one("iframe#cafe_main")
        if iframe and iframe.get("src"):
            iframe_url = urljoin("https://cafe.naver.com", iframe["src"])
            res2 = requests.get(iframe_url, headers=get_headers(), timeout=25, allow_redirects=True)
            if res2.status_code != 200:
                return ""
            soup = BeautifulSoup(res2.text, "html.parser")

        content = soup.select_one("div.se-main-container")
        if not content:
            content = soup.select_one("div#postViewArea") or soup.select_one("div.ContentRenderer")
        if not content:
            return ""

        return content.get_text(" ", strip=True)

    except Exception:
        return ""


# =====================
# ì¤‘ë³µ íŒì •
# =====================
def dup_by_author(df: pd.DataFrame):
    groups = df[df["author"].astype(str).str.len() > 0].groupby("author").indices
    pairs = []
    for author, idxs in groups.items():
        if len(idxs) >= 2:
            idxs = list(idxs)
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    pairs.append((idxs[i], idxs[j], 1.0, f"ì‘ì„±ì ë™ì¼: {author}"))
    return pairs


def dup_by_title(df: pd.DataFrame):
    groups = df.groupby("title_norm").indices
    pairs = []
    for t, idxs in groups.items():
        if t and len(idxs) >= 2:
            idxs = list(idxs)
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    pairs.append((idxs[i], idxs[j], 1.0, "ì œëª© ë™ì¼"))
    return pairs


def dup_by_keywords(df: pd.DataFrame, jaccard_threshold: float = 0.6):
    token_sets = []
    for _, r in df.iterrows():
        token_sets.append(set(simple_tokens(f"{r.get('title','')} {r.get('content','')}")))

    pairs = []
    n = len(token_sets)
    for i in range(n):
        for j in range(i + 1, n):
            a, b = token_sets[i], token_sets[j]
            if not a or not b:
                continue
            score = len(a & b) / len(a | b) if (a | b) else 0.0
            if score >= jaccard_threshold:
                pairs.append((i, j, score, f"í‚¤ì›Œë“œ ì¤‘ë³µ(Jaccard {score:.2f})"))
    return pairs


def dup_by_ai(df: pd.DataFrame, threshold: float = 0.7):
    texts = df["content"].fillna("").astype(str).tolist()
    try:
        vectorizer = TfidfVectorizer(min_df=2)
        tfidf = vectorizer.fit_transform(texts)
    except Exception:
        vectorizer = TfidfVectorizer(min_df=1)
        tfidf = vectorizer.fit_transform(texts)

    sim = cosine_similarity(tfidf)
    pairs = []
    for i in range(len(sim)):
        for j in range(i + 1, len(sim)):
            if sim[i, j] >= threshold:
                pairs.append((i, j, float(sim[i, j]), f"AI ìœ ì‚¬(cos {sim[i,j]:.2f})"))
    return pairs


def build_pairs_table(df: pd.DataFrame, pairs: list[tuple]):
    rows = []
    for i, j, score, reason in pairs:
        rows.append(
            {
                "A_idx": i,
                "A_title": df.loc[i, "title"],
                "A_author": df.loc[i, "author"],
                "A_link": df.loc[i, "link"],
                "B_idx": j,
                "B_title": df.loc[j, "title"],
                "B_author": df.loc[j, "author"],
                "B_link": df.loc[j, "link"],
                "score": round(float(score), 3),
                "reason": reason,
            }
        )
    return pd.DataFrame(rows)


# =====================
# UI
# =====================
st.set_page_config(page_title="í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µê²€ì‚¬", layout="wide")
st.markdown("<style>.block-container{max-width:1400px;}</style>", unsafe_allow_html=True)

st.title("ğŸ“Œ í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µê²€ì‚¬")

# âœ… ë¡œê·¸ì¸(ì¿ í‚¤) ì…ë ¥ ì˜ì—­
st.subheader("ğŸ” ë„¤ì´ë²„ ë¡œê·¸ì¸ (ì¿ í‚¤ ì…ë ¥)")
with st.expander("ì¿ í‚¤ ì…ë ¥ ë°©ë²• / ì…ë ¥ì¹¸ ì—´ê¸°", expanded=True):
    st.markdown(
        """
**ì¤‘ìš”:** ID/ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ì´ ì•„ë‹ˆë¼, **ë¡œê·¸ì¸ëœ ì¿ í‚¤ ê°’ë§Œ ë¶™ì—¬ë„£ëŠ” ë°©ì‹**ì´ì•¼.

### ì¿ í‚¤ ë³µì‚¬í•˜ëŠ” ë°©ë²• (PC í¬ë¡¬ ê¸°ì¤€)
1) ë„¤ì´ë²„ ì¹´í˜ì— ì ‘ì†í•´ì„œ **ë¡œê·¸ì¸**
2) í‚¤ë³´ë“œ `F12` (ê°œë°œìë„êµ¬) ì—´ê¸°
3) ìœ„ íƒ­ì—ì„œ **Application(ì• í”Œë¦¬ì¼€ì´ì…˜)** ì„ íƒ  
   - ì•ˆ ë³´ì´ë©´ `>>` ëˆ„ë¥´ê³  ì°¾ê¸°
4) ì™¼ìª½ ë©”ë‰´ì—ì„œ **Cookies â†’ https://cafe.naver.com**
5) ì˜¤ë¥¸ìª½ í‘œì—ì„œ ì•„ë˜ 2ê°œë¥¼ ì°¾ì•„ì„œ **Valueë¥¼ ë³µì‚¬**
   - `NID_AUT`
   - `NID_SES`

ë³µì‚¬í•œ ê°’ì€ ì•„ë˜ ì¹¸ì— ë¶™ì—¬ë„£ê¸°ë§Œ í•˜ë©´ ë¼.
        """
    )

    nid_aut_in = st.text_input("NID_AUT ê°’", type="password")
    nid_ses_in = st.text_input("NID_SES ê°’", type="password")

    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("âœ… ì¿ í‚¤ ì €ì¥"):
            if not nid_aut_in or not nid_ses_in:
                st.error("NID_AUT, NID_SES ë‘˜ ë‹¤ ì…ë ¥í•´ì•¼ í•´.")
            else:
                st.session_state["naver_cookie"] = f"NID_AUT={nid_aut_in}; NID_SES={nid_ses_in}"
                st.success("ì €ì¥ ì™„ë£Œ! ì´ì œ ìˆ˜ì§‘ì´ ê°€ëŠ¥í•´.")
    with col2:
        if st.button("ğŸ§¹ ì¿ í‚¤ ì‚­ì œ"):
            st.session_state.pop("naver_cookie", None)
            st.success("ì¿ í‚¤ ì‚­ì œ ì™„ë£Œ.")

cookie_ready = bool(st.session_state.get("naver_cookie")) or (
    hasattr(st, "secrets") and st.secrets.get("NID_AUT", "") and st.secrets.get("NID_SES", "")
)

if not cookie_ready:
    st.warning("âš ï¸ Render ë°°í¬ì—ì„œëŠ” ì¿ í‚¤ê°€ ì—†ìœ¼ë©´ ëª©ë¡ì´ 0ê°œë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆì–´. (ë„¤ì´ë²„ê°€ ì„œë²„ IPë¥¼ ë§‰ëŠ” ê²½ìš°)")
st.divider()

# ìƒë‹¨ í† ê¸€
colA, colB, colC, colD, colE = st.columns([1, 1, 1, 1, 1])
with colA:
    opt_original = st.toggle("ğŸ“Œ ì›ë³¸", value=True)
with colB:
    opt_author = st.toggle("ğŸš¨ ì‘ì„±ì ë™ì¼", value=True)
with colC:
    opt_title = st.toggle("ğŸ§· ì œëª© ë™ì¼", value=True)
with colD:
    opt_keyword = st.toggle("ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", value=False)
with colE:
    opt_ai = st.toggle("ğŸ¤– AI ìœ ì‚¬", value=True)

st.divider()

left, right = st.columns([1, 1])
with left:
    target_date = st.date_input("ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ ì„ íƒ (KST ê¸°ì¤€)", kst_today())
with right:
    max_pages = st.number_input("ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜", 1, 200, 10, 1)

with st.expander("âš™ï¸ ì¤‘ë³µ íŒì • ì˜µì…˜", expanded=False):
    ai_threshold = st.slider("ğŸ¤– AI ìœ ì‚¬ ì„ê³„ì¹˜ (cosine)", 0.1, 0.99, 0.70, 0.01)
    kw_threshold = st.slider("ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ ì„ê³„ì¹˜ (Jaccard)", 0.1, 0.99, 0.60, 0.01)

with st.expander("ğŸ§ª ë””ë²„ê·¸ (ë°°í¬ì—ì„œ 0ê°œë©´ í™•ì¸)", expanded=False):
    debug_mode = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ ì¼œê¸°(í˜ì´ì§€ ìƒíƒœ/HTML ì¼ë¶€ í‘œì‹œ)", value=False)

st.divider()

if "df" not in st.session_state:
    st.session_state["df"] = None

run = st.button("ğŸ“¥ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘", type="primary")

if run:
    if not cookie_ready:
        st.error("Render ë°°í¬ì—ì„œëŠ” ì¿ í‚¤ ì—†ì´ ìˆ˜ì§‘ì´ ë§‰í ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ì•„. ìœ„ì—ì„œ ì¿ í‚¤ë¥¼ ì €ì¥í•˜ê³  ë‹¤ì‹œ ëˆŒëŸ¬ì¤˜.")
        st.stop()

    with st.spinner("ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘..."):
        articles, debug_log = collect_article_list(target_date, int(max_pages), debug=debug_mode)

    if debug_mode:
        st.subheader("ğŸ§ª ë””ë²„ê·¸ ë¡œê·¸")
        st.dataframe(pd.DataFrame(debug_log), use_container_width=True)

    if not articles:
        st.error("ëª©ë¡ì—ì„œ í•´ë‹¹ ë‚ ì§œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆì–´. (ì¿ í‚¤ê°€ ë§Œë£Œëê±°ë‚˜, ë„¤ì´ë²„ê°€ ì°¨ë‹¨/HTML ë³€ê²½ ê°€ëŠ¥)")
        st.stop()

    st.success(f"ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")

    progress = st.progress(0.0)
    contents = []
    for i, art in enumerate(articles):
        contents.append(fetch_content(art["link"]))
        progress.progress((i + 1) / len(articles))
        time.sleep(0.15)

    df = pd.DataFrame(articles)
    df["content"] = contents
    st.session_state["df"] = df

df = st.session_state.get("df")
if df is not None:
    st.subheader("âœ… ìˆ˜ì§‘ ê²°ê³¼")

    if opt_original:
        st.dataframe(df[["date", "date_raw", "author", "title", "title_norm", "link"]], use_container_width=True)

    all_pairs = []
    if opt_author:
        all_pairs += dup_by_author(df)
    if opt_title:
        all_pairs += dup_by_title(df)
    if opt_keyword:
        all_pairs += dup_by_keywords(df, float(kw_threshold))
    if opt_ai:
        all_pairs += dup_by_ai(df, float(ai_threshold))

    if not (opt_author or opt_title or opt_keyword or opt_ai):
        st.info("ì¤‘ë³µ ê¸°ì¤€ ë²„íŠ¼ì„ í•˜ë‚˜ ì´ìƒ ì¼œì¤˜.")
        st.stop()

    if all_pairs:
        merged = {}
        for i, j, score, reason in all_pairs:
            key = (min(i, j), max(i, j))
            merged.setdefault(key, {"score": 0.0, "reasons": []})
            merged[key]["score"] = max(merged[key]["score"], float(score))
            merged[key]["reasons"].append(reason)

        final_pairs = [(i, j, v["score"], " / ".join(v["reasons"])) for (i, j), v in merged.items()]
        result_df = build_pairs_table(df, final_pairs).sort_values(["score"], ascending=False)

        st.subheader("âš ï¸ ì¤‘ë³µ ì˜ì‹¬ ê²°ê³¼")
        st.dataframe(result_df, use_container_width=True)
    else:
        st.success("ğŸ‰ ì„ íƒí•œ ê¸°ì¤€ì—ì„œëŠ” ì¤‘ë³µ ì˜ì‹¬ì´ ì—†ì–´!")
