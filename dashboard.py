import re
import time
import traceback
from datetime import datetime, date as date_cls

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait

try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


# =========================
# ëŒ€ìƒ ê²Œì‹œíŒ ê³ ì •
# =========================
CLUB_ID = 28866679
MENU_ID = 178

# âœ… ì•ˆì •ì ì¸ "í´ë˜ì‹ ëª©ë¡"ìœ¼ë¡œ ì§„ì…
BASE_LIST_URL = (
    "https://cafe.naver.com/ArticleList.nhn"
    f"?search.clubid={CLUB_ID}"
    f"&search.menuid={MENU_ID}"
    "&search.boardtype=L"
)

# ë§í¬ëŠ” ì¼€ì´ìŠ¤ê°€ ì„ì—¬ì„œ ë‘˜ ë‹¤ ì§€ì›:
ARTICLEID_RE = re.compile(r"(?:[?&]articleid=(\d+))|(?:/articles/(\d+))")

# ëª©ë¡ì—ì„œ ê¸€ ë§í¬ë¥¼ ì°¾ëŠ” CSS
LINK_CSS = "a[href*='articleid='], a[href*='/articles/']"


# =========================
# ìœ í‹¸
# =========================
def clean(x: str) -> str:
    return (x or "").replace("\u200b", "").strip()


def kst_today() -> date_cls:
    if KST is None:
        return datetime.now().date()
    return datetime.now(KST).date()


def is_time_token(s: str) -> bool:
    return re.fullmatch(r"\d{1,2}:\d{2}", (s or "").strip()) is not None


def extract_time_token(text: str) -> str:
    m = re.search(r"\b(\d{1,2}:\d{2})\b", clean(text))
    return m.group(1) if m else ""


def extract_date_token_any(text: str):
    """
    ëª©ë¡ ë‚ ì§œ í‘œê¸°ê°€ 3ì¢…ë¥˜ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ì„œ ëª¨ë‘ ëŒ€ì‘:
      - 2025.12.16
      - 12.16
      - 2025.12.16. (ë ì )
    return: ("YMD", "2025.12.16") or ("MD", "12.16") or ("", "")
    """
    t = clean(text)
    m1 = re.search(r"\b(20\d{2}\.\d{2}\.\d{2})\.?\b", t)
    if m1:
        return ("YMD", m1.group(1))
    m2 = re.search(r"\b(\d{2}\.\d{2})\b", t)
    if m2:
        return ("MD", m2.group(1))
    return ("", "")


def parse_dot_ymd(s: str):
    try:
        return datetime.strptime(s, "%Y.%m.%d").date()
    except Exception:
        return None


def parse_dot_md(s: str, year: int):
    try:
        mm, dd = s.split(".")
        return date_cls(year, int(mm), int(dd))
    except Exception:
        return None


def build_page_url(page: int) -> str:
    return f"{BASE_LIST_URL}&search.page={page}"


# =========================
# ì œëª© ì •ê·œí™”/í† í°
# =========================
STOPWORDS = {
    "steam", "kakao", "paragon", "pubg",
    "í´ëœ", "í´ëœì›", "ëª¨ì§‘", "í™˜ì˜", "ê°€ì…",
    "ë””ìŠ¤ì½”ë“œ", "discord", "ì„œë²„",
    "ì´ˆë³´", "ì‹ ìƒ", "ì¹œëª©", "ê²½ìŸ", "ì§ì¥ì¸",
    "ì¼ë°˜", "ë­í¬", "ë­ê²œ", "ìŠ¤ì¿¼ë“œ", "ë“€ì˜¤", "ì†”ë¡œ",
    "ë‚´ì „", "ììœ ", "ì´ë²¤íŠ¸", "ì•ˆë‚´", "ê³µì§€",
}


def normalize_title(raw: str) -> str:
    t = clean(raw)

    # ë ëŒ“ê¸€ìˆ˜ ì œê±°
    t = re.sub(r"\s*\[\s*\d+\s*\]\s*$", "", t)
    t = re.sub(r"\s*\(\s*\d+\s*\)\s*$", "", t)

    # [Steam] ê°™ì€ íƒœê·¸ ì œê±°
    t = re.sub(r"\[[^\]]{1,30}\]", " ", t)

    # LV / ë‚˜ì´/ë²”ìœ„ íŒ¨í„´ ì œê±°
    t = re.sub(r"\bLv\.?\s*\d+\b", " ", t, flags=re.IGNORECASE)
    t = re.sub(r"\b\d{1,2}\s*~\s*\d{1,2}\b", " ", t)
    t = re.sub(r"\b\d{1,2}\s*ì„¸\b", " ", t)

    # url ì œê±°
    t = re.sub(r"https?://\S+", " ", t)

    # ì´ëª¨ì§€/ê¸°í˜¸ ì œê±° (í•œ/ì˜/ìˆ«ì/ê³µë°±ë§Œ ìœ ì§€)
    t = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", t)

    # ìˆ«ì ë‹¨ë… ì œê±°
    t = re.sub(r"\b\d+\b", " ", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t


def tokenize(text: str):
    t = normalize_title(text)
    toks = re.findall(r"[a-z]+|[ê°€-í£]+", t)
    toks = [x for x in toks if len(x) >= 2]
    toks = [x for x in toks if x not in STOPWORDS]
    return toks


# =========================
# Selenium
# =========================
def guess_chrome_binary():
    # Render/Ubuntuì—ì„œ í”í•œ ê²½ë¡œë“¤
    candidates = [
        "/usr/bin/chromium",
        "/usr/bin/chromium-browser",
        "/usr/bin/google-chrome",
        "/usr/bin/google-chrome-stable",
    ]
    return candidates


def make_driver(headless: bool = True) -> webdriver.Chrome:
    opts = Options()

    # âœ… ì•ˆì • ì˜µì…˜
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1400,900")
    opts.add_argument("--lang=ko-KR")

    # âœ… ë” ì•ˆì •ì ìœ¼ë¡œ (Renderì—ì„œ ìœ ìš©)
    opts.add_argument("--disable-background-networking")
    opts.add_argument("--disable-background-timer-throttling")
    opts.add_argument("--disable-renderer-backgrounding")
    opts.add_argument("--disable-features=Translate,BackForwardCache,AcceptCHFrame")
    opts.add_argument("--disable-extensions")
    opts.add_argument("--disable-notifications")

    # âœ… ì†ë„/ì•ˆì •
    opts.page_load_strategy = "eager"

    if headless:
        opts.add_argument("--headless=new")

    # âœ… ì´ë¯¸ì§€ ì°¨ë‹¨(ì†ë„â†‘)
    opts.add_experimental_option("prefs", {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
    })

    # âœ… ìë™í™” íƒì§€ ì™„í™”(ê°€ëŠ¥í•œ ë²”ìœ„)
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)

    # âœ… í¬ë¡¬ ë°”ì´ë„ˆë¦¬ ê²½ë¡œ ì§€ì •(ìˆìœ¼ë©´)
    for p in guess_chrome_binary():
        try:
            import os
            if os.path.exists(p):
                opts.binary_location = p
                break
        except Exception:
            pass

    # Selenium Managerê°€ ë“œë¼ì´ë²„ë¥¼ ì•Œì•„ì„œ ë§ì¶°ì¤Œ(ì˜¨ë¼ì¸ í™˜ê²½ì—ì„œ ì•ˆì •)
    service = Service()
    driver = webdriver.Chrome(service=service, options=opts)
    driver.implicitly_wait(0.5)

    # navigator.webdriver ìˆ¨ê¹€(ê°€ëŠ¥í•œ ë²”ìœ„)
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"}
        )
    except Exception:
        pass

    return driver


def switch_to_cafe_main_iframe(driver) -> bool:
    try:
        driver.switch_to.default_content()
        iframes = driver.find_elements(By.ID, "cafe_main")
        if iframes:
            driver.switch_to.frame("cafe_main")
            return True
    except Exception:
        pass
    return False


def wait_list_loaded(driver):
    wait = WebDriverWait(driver, 25)

    def has_links_in_current_doc(d):
        return len(d.find_elements(By.CSS_SELECTOR, LINK_CSS)) > 0

    # 1) iframe ë¨¼ì €
    if switch_to_cafe_main_iframe(driver):
        try:
            wait.until(has_links_in_current_doc)
            return
        except Exception:
            pass

    # 2) default contentì—ì„œ ë‹¤ì‹œ ì²´í¬
    try:
        driver.switch_to.default_content()
    except Exception:
        pass

    wait.until(has_links_in_current_doc)


def is_notice_row(row_text: str, row_el) -> bool:
    t = clean(row_text)
    lines = [x.strip() for x in t.split("\n") if x.strip()]
    if any(x == "ê³µì§€" for x in lines):
        return True
    try:
        if row_el is not None and len(row_el.find_elements(By.XPATH, ".//*[normalize-space()='ê³µì§€']")) > 0:
            return True
    except Exception:
        pass
    return False


def pick_row_author(row_text: str, title: str) -> str:
    t = clean(row_text)
    lines = [x.strip() for x in t.split("\n") if x.strip()]
    lines = [x for x in lines if x != title]
    lines = [x for x in lines if not is_time_token(x)]
    # ë‚ ì§œí˜• ì œê±°(ë‘˜ ë‹¤)
    lines = [x for x in lines if not re.fullmatch(r"20\d{2}\.\d{2}\.\d{2}\.?", x)]
    lines = [x for x in lines if not re.fullmatch(r"\d{2}\.\d{2}", x)]
    bad = ["ì¡°íšŒ", "ì¢‹ì•„ìš”", "ëŒ“ê¸€", "ëŒ“ê¸€ìˆ˜"]
    lines = [x for x in lines if not any(b in x for b in bad)]
    lines = [x for x in lines if x != "ê³µì§€"]
    for x in lines:
        if 1 <= len(x) <= 30:
            return x
    return ""


def extract_article_id_from_href(href: str) -> str:
    m = ARTICLEID_RE.search(href or "")
    if not m:
        return ""
    return m.group(1) or m.group(2) or ""


# =========================
# "í•œ í˜ì´ì§€"ë§Œ ìˆ˜ì§‘ (ì¤‘ìš”: ê¸´ ì‘ì—…ì„ ëŠì–´ì„œ ì‹¤í–‰)
# =========================
def collect_one_page(driver, target_date: date_cls, page: int, pause: float):
    """
    return:
      collected_dict (href -> row dict),
      page_matches (int),
      saw_any_row (bool)
    """
    today = kst_today()
    is_today = (target_date == today)
    target_iso = target_date.strftime("%Y-%m-%d")

    driver.get(build_page_url(page))
    wait_list_loaded(driver)
    time.sleep(pause)

    rows = driver.find_elements(By.CSS_SELECTOR, "tr")
    if len(rows) < 5:
        rows = driver.find_elements(By.CSS_SELECTOR, "li") + rows

    collected = {}
    page_matches = 0
    saw_any = False

    for row in rows:
        try:
            row_text = clean(row.text)
            if not row_text:
                continue
            saw_any = True
            if is_notice_row(row_text, row):
                continue

            links = row.find_elements(By.CSS_SELECTOR, LINK_CSS)
            if not links:
                continue

            a = links[0]
            href = clean(a.get_attribute("href"))
            if not href:
                continue

            article_id = extract_article_id_from_href(href)
            if not article_id:
                continue

            # ì œëª©
            title_raw = clean(a.text)
            if not title_raw:
                lines = [x.strip() for x in row_text.split("\n") if x.strip()]
                title_raw = lines[0] if lines else ""
            if not title_raw:
                continue

            hhmm = extract_time_token(row_text)
            dtype, dtoken = extract_date_token_any(row_text)

            # âœ… ë‚ ì§œ ë§¤ì¹­ ë¡œì§ (ì˜¤ëŠ˜=ì‹œê°„, ê³¼ê±°=ë‚ ì§œ(YYYY.MM.DD ë˜ëŠ” MM.DD))
            if is_today:
                if not hhmm:
                    continue
                date_raw = hhmm
            else:
                # ê³¼ê±°ì¸ë° ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš°ë„ ê°€ë” ìˆìŒ(ìµœê·¼ê¸€)
                # â†’ MM.DD í‘œê¸°ê¹Œì§€ ë°›ì•„ì„œ target_dateì™€ ë§¤ì¹­ë˜ë©´ í†µê³¼
                d_obj = None
                if dtype == "YMD":
                    d_obj = parse_dot_ymd(dtoken)
                elif dtype == "MD":
                    d_obj = parse_dot_md(dtoken, target_date.year)

                if d_obj != target_date:
                    continue

                date_raw = dtoken

            collected[href] = {
                "date": target_iso,
                "date_raw": date_raw,
                "author": pick_row_author(row_text, title_raw),
                "title": title_raw,
                "title_norm": normalize_title(title_raw),
                "link": href,
            }
            page_matches += 1
        except Exception:
            continue

    return collected, page_matches, saw_any


# =========================
# ì¤‘ë³µ/ìœ ì‚¬
# =========================
def compute_author_dups(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=["date", "author", "count"])
    a = df.groupby(["date", "author"]).size().reset_index(name="count")
    return a[a["count"] >= 2].sort_values(by="count", ascending=False)


def compute_exact_dups(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=df.columns)
    return df[df.duplicated(subset=["date", "title_norm"], keep=False)].copy()


def compute_keyword_groups(df: pd.DataFrame, min_count: int = 2):
    if df.empty:
        return pd.DataFrame(columns=["keyword", "count", "examples"])

    tokens_list = [tokenize(t) for t in df["title"].fillna("").astype(str).tolist()]

    inv = {}
    for idx, toks in enumerate(tokens_list):
        for tok in set(toks):
            inv.setdefault(tok, []).append(idx)

    rows = []
    for kw, idxs in inv.items():
        if len(idxs) >= min_count:
            ex = [df.iloc[i]["title"] for i in idxs[:3]]
            rows.append({"keyword": kw, "count": len(idxs), "examples": " | ".join(ex)})

    out = pd.DataFrame(rows)
    return out.sort_values(by=["count", "keyword"], ascending=[False, True]) if not out.empty else out


def compute_ai_similar(df: pd.DataFrame, threshold: float = 0.78) -> pd.DataFrame:
    cols = ["title_a", "title_b", "similarity", "link_a", "link_b"]
    if df.empty or len(df) < 2:
        return pd.DataFrame(columns=cols)

    titles_raw = df["title"].fillna("").astype(str).tolist()
    titles = df["title_norm"].fillna("").astype(str).tolist()
    links = df["link"].fillna("").astype(str).tolist()

    vec_w = TfidfVectorizer(analyzer="word", ngram_range=(1, 2), min_df=1)
    Xw = vec_w.fit_transform(titles)

    vec_c = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 5), min_df=1)
    Xc = vec_c.fit_transform(titles)

    Mw = cosine_similarity(Xw)
    Mc = cosine_similarity(Xc)
    M = 0.55 * Mw + 0.45 * Mc

    rows = []
    n = len(titles)
    for i in range(n):
        for j in range(i + 1, n):
            s = float(M[i, j])
            if s >= threshold:
                rows.append({
                    "title_a": titles_raw[i],
                    "title_b": titles_raw[j],
                    "similarity": round(s, 3),
                    "link_a": links[i],
                    "link_b": links[j],
                })

    out = pd.DataFrame(rows, columns=cols)
    return out.sort_values(by="similarity", ascending=False) if not out.empty else out


# =========================
# Streamlit (ì•ˆ íŠ•ê¸°ê²Œ êµ¬ì¡°)
# =========================
st.set_page_config(page_title="menu=178 ìˆ˜ì§‘/ì¤‘ë³µ", layout="wide")
st.title("ğŸ° í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µ ê²Œì‹œê¸€ ì²´í¬ (menu=178)")

# ---- session init
if "running" not in st.session_state:
    st.session_state.running = False
if "driver" not in st.session_state:
    st.session_state.driver = None
if "collected" not in st.session_state:
    st.session_state.collected = {}   # href -> row dict
if "page" not in st.session_state:
    st.session_state.page = 1
if "no_match_pages" not in st.session_state:
    st.session_state.no_match_pages = 0
if "last_error" not in st.session_state:
    st.session_state.last_error = ""
if "debug_events" not in st.session_state:
    st.session_state.debug_events = []


def debug_log(msg: str):
    st.session_state.debug_events.append(f"{datetime.now().strftime('%H:%M:%S')}  {msg}")
    st.session_state.debug_events = st.session_state.debug_events[-300:]


def stop_and_cleanup():
    st.session_state.running = False
    st.session_state.no_match_pages = 0
    st.session_state.page = 1
    # driverëŠ” ì›í•˜ë©´ ìœ ì§€ ê°€ëŠ¥í•˜ì§€ë§Œ, ì•ˆì •ì ìœ¼ë¡œëŠ” ë‹«ëŠ” í¸ì´ ì¢‹ìŒ
    try:
        if st.session_state.driver is not None:
            st.session_state.driver.quit()
    except Exception:
        pass
    st.session_state.driver = None


with st.expander("ì„¤ì •", expanded=True):
    c1, c2, c3, c4, c5 = st.columns([1.2, 1.0, 1.2, 1.2, 1.2])
    with c1:
        target_date = st.date_input("ë‚ ì§œ ì„ íƒ", value=kst_today())
    with c2:
        headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤", value=True)
    with c3:
        max_pages = st.number_input("ìµœëŒ€ í˜ì´ì§€", min_value=1, max_value=500, value=120, step=5)
    with c4:
        stop_no_match_pages = st.number_input("ì—°ì† 0í˜ì´ì§€ë©´ ì¢…ë£Œ", min_value=1, max_value=10, value=3, step=1)
    with c5:
        pause = st.number_input("í˜ì´ì§€ ëŒ€ê¸°(ì´ˆ)", min_value=0.05, max_value=2.00, value=0.25, step=0.05)

    c6, c7, c8 = st.columns([1.2, 1.2, 1.2])
    with c6:
        pages_per_tick = st.number_input("í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€(ê¶Œì¥ 1~3)", min_value=1, max_value=10, value=2, step=1)
    with c7:
        keyword_min_count = st.number_input("í‚¤ì›Œë“œ ì¤‘ë³µ ìµœì†Œ ê±´ìˆ˜", min_value=2, max_value=20, value=2, step=1)
    with c8:
        sim_threshold = st.slider("AI ìœ ì‚¬ë„ ê¸°ì¤€", 0.50, 0.99, 0.78, 0.01)

st.divider()

btn1, btn2, btn3 = st.columns([1, 1, 1])
with btn1:
    start = st.button("ìˆ˜ì§‘ ì‹œì‘", use_container_width=True, disabled=st.session_state.running)
with btn2:
    stop = st.button("ì¤‘ì§€", use_container_width=True, disabled=not st.session_state.running)
with btn3:
    reset = st.button("ì´ˆê¸°í™”(ë°ì´í„° ì‚­ì œ)", use_container_width=True)

if reset:
    stop_and_cleanup()
    st.session_state.collected = {}
    st.session_state.last_error = ""
    st.session_state.debug_events = []
    st.success("ì´ˆê¸°í™” ì™„ë£Œ")

if stop:
    stop_and_cleanup()
    st.warning("ì¤‘ì§€ë¨")

if start:
    # ìƒˆ ì‹¤í–‰
    stop_and_cleanup()
    st.session_state.collected = {}
    st.session_state.last_error = ""
    st.session_state.running = True
    debug_log("START pressed")


# ---- Running loop (ì§§ê²Œ ëŠì–´ì„œ ì‹¤í–‰)
progress_box = st.empty()
status_box = st.empty()

if st.session_state.running:
    try:
        if st.session_state.driver is None:
            debug_log("Creating driver...")
            st.session_state.driver = make_driver(headless=headless)
            debug_log("Driver created.")

        # ì´ë²ˆ tickì— ëª‡ í˜ì´ì§€ ì²˜ë¦¬
        pages_done = 0
        tick_start = time.time()

        while pages_done < int(pages_per_tick) and st.session_state.page <= int(max_pages):
            p = st.session_state.page
            progress_box.info(f"ìˆ˜ì§‘ ì¤‘... page={p} / max={int(max_pages)}  (í˜„ì¬ ìˆ˜ì§‘ {len(st.session_state.collected)}ê°œ)")
            debug_log(f"Collecting page {p}")

            collected, page_matches, saw_any = collect_one_page(
                st.session_state.driver, target_date=target_date, page=p, pause=float(pause)
            )

            # ë³‘í•©
            for k, v in collected.items():
                st.session_state.collected[k] = v

            if page_matches > 0:
                st.session_state.no_match_pages = 0
            else:
                st.session_state.no_match_pages += 1

            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if st.session_state.no_match_pages >= int(stop_no_match_pages):
                debug_log("Stop condition met: consecutive no-match pages")
                st.session_state.running = False
                break

            st.session_state.page += 1
            pages_done += 1

            # ë„ˆë¬´ ì˜¤ë˜ ë¶™ì¡ì§€ ì•Šê¸°(ì„¸ì…˜ ë¦¬ì…‹ ë°©ì§€)
            if time.time() - tick_start > 12:
                debug_log("Tick time budget reached, yielding to Streamlit rerun")
                break

        # ì¢…ë£Œ ì¡°ê±´: ìµœëŒ€ í˜ì´ì§€ ë„ë‹¬
        if st.session_state.page > int(max_pages):
            debug_log("Reached max_pages. Stopping.")
            st.session_state.running = False

        # ì•„ì§ runningì´ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ tick ì§„í–‰
        if st.session_state.running:
            status_box.warning("ê³„ì† ìˆ˜ì§‘ ì¤‘... ì ì‹œ í›„ ìë™ìœ¼ë¡œ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            time.sleep(0.2)
            st.rerun()
        else:
            # ëë‚¬ìœ¼ë©´ ë“œë¼ì´ë²„ ì •ë¦¬
            try:
                if st.session_state.driver is not None:
                    st.session_state.driver.quit()
            except Exception:
                pass
            st.session_state.driver = None
            status_box.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(st.session_state.collected)}ê°œ")

    except Exception:
        st.session_state.last_error = traceback.format_exc()
        debug_log("ERROR: " + st.session_state.last_error.splitlines()[-1] if st.session_state.last_error else "ERROR")
        st.session_state.running = False
        try:
            if st.session_state.driver is not None:
                st.session_state.driver.quit()
        except Exception:
            pass
        st.session_state.driver = None
        st.error("ìˆ˜ì§‘ ì˜¤ë¥˜")
        st.code(st.session_state.last_error)

# ---- DataFrame
df = pd.DataFrame(list(st.session_state.collected.values()))
if not df.empty:
    df = df.drop_duplicates(subset=["link"]).copy()
    # date_rawê°€ ì‹œê°„/ë‚ ì§œ í˜¼í•©ì´ë¼ ì •ë ¬ì€ ë¬¸ìì—´ ê¸°ì¤€
    df = df.sort_values(by="date_raw", ascending=False)

author_dups = compute_author_dups(df)
exact_dups = compute_exact_dups(df)
keyword_groups = compute_keyword_groups(df, min_count=int(keyword_min_count))
ai_similar = compute_ai_similar(df, threshold=float(sim_threshold))

tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ“Œ ì›ë³¸", "ğŸš¨ ì‘ì„±ì ë™ì¼", "ğŸ§¨ ì œëª© ë™ì¼", "ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", "ğŸ¤– AI ìœ ì‚¬", "ğŸ§ª ë””ë²„ê·¸"])

with tab1:
    st.dataframe(df, use_container_width=True)

with tab2:
    st.dataframe(author_dups if not author_dups.empty else pd.DataFrame(), use_container_width=True)
    if author_dups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")

with tab3:
    st.dataframe(exact_dups if not exact_dups.empty else pd.DataFrame(), use_container_width=True)
    if exact_dups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")

with tab4:
    st.dataframe(keyword_groups if not keyword_groups.empty else pd.DataFrame(), use_container_width=True)
    if keyword_groups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")

with tab5:
    st.dataframe(ai_similar if not ai_similar.empty else pd.DataFrame(), use_container_width=True)
    if ai_similar.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")

with tab6:
    st.write("ìµœê·¼ ë””ë²„ê·¸ ì´ë²¤íŠ¸(ìµœëŒ€ 300ì¤„):")
    st.code("\n".join(st.session_state.debug_events[-300:]) if st.session_state.debug_events else "(ì—†ìŒ)")
    if st.session_state.last_error:
        st.write("ë§ˆì§€ë§‰ ì˜¤ë¥˜:")
        st.code(st.session_state.last_error)
