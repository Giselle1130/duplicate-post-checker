import re
import time
import traceback
from datetime import datetime, date as date_cls
from urllib.parse import urljoin

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


# =========================
# Streamlit ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="ğŸ° í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µ ê²Œì‹œê¸€ ì²´í¬", layout="wide")
st.title("ğŸ° í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µ ê²Œì‹œê¸€ ì²´í¬")


# =========================
# ëŒ€ìƒ ê²Œì‹œíŒ ê³ ì •
# =========================
CLUB_ID = 28866679
MENU_ID = 178
BASE = "https://cafe.naver.com"

# í´ë˜ì‹ ëª©ë¡
BASE_LIST_URL = (
    "https://cafe.naver.com/ArticleList.nhn"
    f"?search.clubid={CLUB_ID}"
    f"&search.menuid={MENU_ID}"
    "&search.boardtype=L"
)

# ê¸€ ë§í¬ íŒ¨í„´ (ë‘˜ ë‹¤)
LINK_CSS = "a[href*='articleid='], a[href*='/articles/']"
ARTICLEID_RE = re.compile(r"(?:[?&]articleid=(\d+))|(?:/articles/(\d+))")

# ìƒì„¸ì—ì„œ ì‘ì„±ì¼ í›„ë³´ ì…€ë ‰í„° (UI ë³€í™” ëŒ€ë¹„)
DETAIL_DATE_SELECTORS = [
    "span.date",
    ".article_info .date",
    ".ArticleTopInfo__date",
    ".ArticleTopInfo .date",
    "p.date",
    "span._articleTime",
]


# =========================
# ìœ í‹¸
# =========================
def clean(x: str) -> str:
    return (x or "").replace("\u200b", "").strip()


def kst_today() -> date_cls:
    if KST is None:
        return datetime.now().date()
    return datetime.now(KST).date()


def build_page_url(page: int) -> str:
    return f"{BASE_LIST_URL}&search.page={page}"


def extract_article_id(href: str) -> str:
    m = ARTICLEID_RE.search(href or "")
    if not m:
        return ""
    return m.group(1) or m.group(2) or ""


def parse_detail_datetime_text(raw: str):
    s = clean(raw)

    # 2025.12.16. 23:58
    m = re.search(r"(\d{4})\.(\d{1,2})\.(\d{1,2})\.\s*(\d{1,2}):(\d{2})", s)
    if m:
        y, mo, d, hh, mm = map(int, m.groups())
        return datetime(y, mo, d, hh, mm)

    # 2025.12.16. ì˜¤í›„ 11:58 / ì˜¤ì „ 9:02
    m = re.search(r"(\d{4})\.(\d{1,2})\.(\d{1,2})\.\s*(ì˜¤ì „|ì˜¤í›„)\s*(\d{1,2}):(\d{2})", s)
    if m:
        y, mo, d = map(int, m.group(1, 2, 3))
        ampm = m.group(4)
        hh = int(m.group(5))
        mm = int(m.group(6))
        if ampm == "ì˜¤í›„" and hh != 12:
            hh += 12
        if ampm == "ì˜¤ì „" and hh == 12:
            hh = 0
        return datetime(y, mo, d, hh, mm)

    return None


# =========================
# ì œëª© ì •ê·œí™”/í† í°
# =========================
STOPWORDS = {
    "steam", "kakao", "paragon", "pubg",
    "í´ëœ", "í´ëœì›", "ëª¨ì§‘", "í™˜ì˜", "ê°€ì…",
    "ë””ìŠ¤ì½”ë“œ", "discord", "ì„œë²„",
    "ì´ˆë³´", "ì‹ ìƒ", "ì¹œëª©", "ê²½ìŸ", "ì§ì¥ì¸",
    "ì¼ë°˜", "ë­í¬", "ë­ê²œ", "ìŠ¤ì¿¼ë“œ", "ë“€ì˜¤", "ì†”ë¡œ",
    "ë‚´ì „", "ììœ ", "ì´ë²¤íŠ¸", "ì•ˆë‚´", "ê³µì§€",
}


def normalize_title(raw: str) -> str:
    t = clean(raw)
    t = re.sub(r"\s*\[\s*\d+\s*\]\s*$", "", t)
    t = re.sub(r"\s*\(\s*\d+\s*\)\s*$", "", t)
    t = re.sub(r"\[[^\]]{1,30}\]", " ", t)
    t = re.sub(r"\bLv\.?\s*\d+\b", " ", t, flags=re.IGNORECASE)
    t = re.sub(r"\b\d{1,2}\s*~\s*\d{1,2}\b", " ", t)
    t = re.sub(r"\b\d{1,2}\s*ì„¸\b", " ", t)
    t = re.sub(r"https?://\S+", " ", t)
    t = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", t)
    t = re.sub(r"\b\d+\b", " ", t)
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t


def tokenize(text: str):
    t = normalize_title(text)
    toks = re.findall(r"[a-z]+|[ê°€-í£]+", t)
    toks = [x for x in toks if len(x) >= 2]
    toks = [x for x in toks if x not in STOPWORDS]
    return toks


# =========================
# Selenium (Selenium Manager)
# =========================
def make_driver(headless: bool = True) -> webdriver.Chrome:
    opts = Options()
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1400,900")
    opts.page_load_strategy = "eager"
    if headless:
        opts.add_argument("--headless=new")

    # ë¦¬ëˆ…ìŠ¤/Render ì•ˆì •ì„±
    opts.add_argument("--remote-debugging-port=9222")

    # ì´ë¯¸ì§€ ì°¨ë‹¨
    opts.add_experimental_option("prefs", {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
    })

    driver = webdriver.Chrome(options=opts)  # âœ… Selenium Manager
    driver.implicitly_wait(0.2)
    return driver


def switch_to_cafe_iframe(driver) -> bool:
    """
    cafe_main iframeì´ ID/NAME í˜•íƒœë¡œ ì„ì—¬ ë‚˜ì™€ì„œ ë‘˜ ë‹¤ ì‹œë„.
    """
    try:
        driver.switch_to.default_content()
    except Exception:
        pass

    # 1) NAME
    try:
        WebDriverWait(driver, 2).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "cafe_main")))
        return True
    except Exception:
        pass

    # 2) ID
    try:
        driver.switch_to.default_content()
        iframes = driver.find_elements(By.ID, "cafe_main")
        if iframes:
            driver.switch_to.frame("cafe_main")
            return True
    except Exception:
        pass

    return False


def wait_any_links(driver, timeout=12):
    """
    iframe ì•ˆ/ë°–ì—ì„œ ê¸€ ë§í¬ê°€ ìƒê¸¸ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼.
    """
    wait = WebDriverWait(driver, timeout)

    def has_links_in_current(d):
        return len(d.find_elements(By.CSS_SELECTOR, LINK_CSS)) > 0

    # iframe ë¨¼ì €
    if switch_to_cafe_iframe(driver):
        try:
            wait.until(has_links_in_current)
            return True
        except Exception:
            pass

    # ë°–ì—ì„œ ë‹¤ì‹œ
    try:
        driver.switch_to.default_content()
    except Exception:
        pass

    try:
        wait.until(has_links_in_current)
        return True
    except Exception:
        return False


def collect_links_from_current_page(driver):
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸€ ë§í¬ë“¤ì„ ìµœëŒ€í•œ ë‹¨ìˆœ/ê°•í•˜ê²Œ ìˆ˜ì§‘.
    row(tr/li) êµ¬ì¡° ì•ˆ ë¯¿ê³  a ë§í¬ë§Œ ì‹¹ ê¸ìŒ.
    """
    links = driver.find_elements(By.CSS_SELECTOR, LINK_CSS)
    out = []

    for a in links:
        try:
            href = clean(a.get_attribute("href"))
            if not href:
                continue
            if href.startswith("/"):
                href = urljoin(BASE, href)

            aid = extract_article_id(href)
            if not aid:
                continue

            title = clean(a.text)
            if not title:
                # titleì´ a ë°–ì— ìˆì„ ìˆ˜ ìˆìŒ: ì£¼ë³€ í…ìŠ¤íŠ¸ë¡œ ë³´ì¡°
                title = clean(a.get_attribute("title"))

            # ì‘ì„±ì ì¶”ì¶œì€ í™˜ê²½ë§ˆë‹¤ ë„ˆë¬´ í”ë“¤ë ¤ì„œ "ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³  ì—†ìœ¼ë©´ ê³µë°±"
            author = ""
            try:
                # ê·¼ì²˜ì— nickname ê°™ì€ê²Œ ìˆìœ¼ë©´ ì½ê¸°
                parent = a.find_element(By.XPATH, "./ancestor::*[self::tr or self::li][1]")
                cand = parent.text.split("\n")
                cand = [c.strip() for c in cand if c.strip()]
                # ì œëª© ì œì™¸í•˜ê³  ì§§ì€ í…ìŠ¤íŠ¸ ì¤‘ 1ê°œ ê³¨ë¼ë³´ê¸°
                cand2 = [c for c in cand if c != title and len(c) <= 30 and "ì¡°íšŒ" not in c and "ëŒ“ê¸€" not in c]
                author = cand2[0] if cand2 else ""
            except Exception:
                author = ""

            out.append((aid, href, title, author))
        except Exception:
            continue

    # articleid ê¸°ì¤€ unique
    uniq = {}
    for aid, href, title, author in out:
        if aid not in uniq:
            uniq[aid] = (href, title, author)
    return uniq


def get_article_datetime_strict(driver, href: str, pause: float = 0.05):
    try:
        driver.get(href)
        time.sleep(pause)

        switch_to_cafe_iframe(driver)
        wait = WebDriverWait(driver, 10)

        for css in DETAIL_DATE_SELECTORS:
            try:
                el = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, css)))
                dt = parse_detail_datetime_text(el.text)
                if dt:
                    return dt
            except Exception:
                continue

        # ìµœí›„ì˜ ìˆ˜ë‹¨: ì†ŒìŠ¤ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        src = driver.page_source
        m = re.search(r"(\d{4}\.\d{1,2}\.\d{1,2}\.\s*(?:ì˜¤ì „|ì˜¤í›„)\s*\d{1,2}:\d{2})", src)
        if m:
            dt = parse_detail_datetime_text(m.group(1))
            if dt:
                return dt

        m = re.search(r"(\d{4}\.\d{1,2}\.\d{1,2}\.\s*\d{1,2}:\d{2})", src)
        if m:
            dt = parse_detail_datetime_text(m.group(1))
            if dt:
                return dt

    except Exception:
        return None

    return None


# =========================
# ì§„í–‰/ì¤‘ì§€/ë””ë²„ê·¸ ìƒíƒœë¨¸ì‹ 
# =========================
def ensure_state():
    ss = st.session_state
    ss.setdefault("running", False)
    ss.setdefault("phase", "idle")  # idle | collect | validate | done
    ss.setdefault("driver", None)
    ss.setdefault("logs", [])
    ss.setdefault("page", 1)
    ss.setdefault("seen_ids", set())      # í˜ì´ì§€ ì¤‘ë³µ ê°ì§€ìš©
    ss.setdefault("candidates", {})       # articleid -> info
    ss.setdefault("validate_ids", [])
    ss.setdefault("validate_i", 0)
    ss.setdefault("collected", {})        # articleid -> info
    ss.setdefault("last_url", "")
    ss.setdefault("posts", [])


def log(msg: str):
    st.session_state.logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")


def reset_job():
    ss = st.session_state
    try:
        if ss.driver is not None:
            ss.driver.quit()
    except Exception:
        pass

    ss.running = False
    ss.phase = "idle"
    ss.driver = None
    ss.logs = []
    ss.page = 1
    ss.seen_ids = set()
    ss.candidates = {}
    ss.validate_ids = []
    ss.validate_i = 0
    ss.collected = {}
    ss.last_url = ""
    ss.posts = []


def start_job(target_date: date_cls, headless: bool, max_pages: int, pause: float):
    reset_job()
    ss = st.session_state
    ss.target_date = target_date
    ss.headless = headless
    ss.max_pages = int(max_pages)
    ss.pause = float(pause)

    ss.driver = make_driver(headless=headless)
    ss.phase = "collect"
    ss.running = True
    log(f"ì‹œì‘: target_date={target_date} max_pages={max_pages}")


def stop_job():
    st.session_state.running = False
    log("ì¤‘ì§€(ì‚¬ìš©ì ìš”ì²­)")


def finalize_job():
    ss = st.session_state
    df = pd.DataFrame(list(ss.collected.values()))
    if not df.empty:
        df = df.drop_duplicates(subset=["link"]).copy()
        df = df.sort_values(by="date_detail", ascending=False)
    ss.posts = df.to_dict("records")
    ss.phase = "done"
    ss.running = False
    log(f"ì™„ë£Œ: ìµœì¢… {len(ss.posts)}ê°œ")


def step_collect():
    """
    ëª©ë¡ì€ 'í˜ì´ì§€ 1~max_pages'ê¹Œì§€ë§Œ,
    page2ë¶€í„° ìƒˆ ê¸€ì´ í•˜ë‚˜ë„ ì•ˆ ëŠ˜ì–´ë‚˜ë©´ ì¦‰ì‹œ ì¢…ë£Œ.
    """
    ss = st.session_state
    d = ss.driver

    pages_per_step = int(ss.pages_per_step)
    processed = 0

    while ss.page <= ss.max_pages and processed < pages_per_step and ss.running:
        url = build_page_url(ss.page)
        ss.last_url = url
        log(f"[ëª©ë¡] page={ss.page}")

        try:
            d.get(url)
            ok = wait_any_links(d, timeout=12)
            time.sleep(ss.pause)

            if not ok:
                # ë§í¬ê°€ ì•„ì˜ˆ ì•ˆ ëœ¨ë©´ ë°”ë¡œ ì¢…ë£Œ(ë” ëŒ ì˜ë¯¸ ì—†ìŒ)
                log("ëª©ë¡ì—ì„œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨ â†’ ì¢…ë£Œ")
                ss.page = ss.max_pages + 1
                break

            page_links = collect_links_from_current_page(d)
            page_ids = set(page_links.keys())

            # page2ë¶€í„° "ì™„ì „íˆ ê°™ì€ ëª©ë¡"ì´ë©´ ë” ëŒ í•„ìš” ì—†ìŒ
            if ss.page >= 2 and page_ids and page_ids.issubset(ss.seen_ids):
                log("ìƒˆ ê¸€ì´ ë” ì´ìƒ ì—†ìŒ(ì¤‘ë³µ í˜ì´ì§€) â†’ ì¢…ë£Œ")
                ss.page = ss.max_pages + 1
                break

            before = len(ss.candidates)
            for aid, (href, title, author) in page_links.items():
                ss.seen_ids.add(aid)
                if aid not in ss.candidates:
                    ss.candidates[aid] = {
                        "date": ss.target_date.strftime("%Y-%m-%d"),
                        "date_raw": f"page={ss.page}",
                        "author": author,
                        "title": title,
                        "title_norm": normalize_title(title),
                        "link": href,
                    }
            after = len(ss.candidates)
            log(f"í›„ë³´ ëˆ„ì : {after} (ì´ë²ˆ í˜ì´ì§€ ì‹ ê·œ {after - before})")

            # page2ë¶€í„° ì‹ ê·œê°€ 0ì´ë©´ ì¢…ë£Œ (ë¬´ì•¼ ìƒí™©: ê¸€ 20ê°œ ë¯¸ë§Œ + 1í˜ì´ì§€)
            if ss.page >= 2 and (after - before) == 0:
                log("page>=2 ì‹ ê·œ 0 â†’ ì¢…ë£Œ")
                ss.page = ss.max_pages + 1
                break

        except Exception as e:
            log(f"ëª©ë¡ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            ss.page = ss.max_pages + 1
            break

        ss.page += 1
        processed += 1

    # ëª©ë¡ ë‹¨ê³„ ë â†’ ìƒì„¸ ê²€ì¦
    if ss.page > ss.max_pages:
        ss.validate_ids = list(ss.candidates.keys())
        ss.validate_i = 0
        ss.phase = "validate"
        log(f"ìƒì„¸ ê²€ì¦ ì‹œì‘: í›„ë³´ {len(ss.validate_ids)}ê°œ")


def step_validate():
    """
    í›„ë³´(ëŒ€ê°œ 20ê°œ ë¯¸ë§Œ)ë§Œ ìƒì„¸ ì§„ì…í•´ì„œ ì‘ì„±ì¼ í™•ì¸.
    ì„ íƒ ë‚ ì§œì™€ ì •í™•íˆ ê°™ì€ ê¸€ë§Œ í†µê³¼.
    """
    ss = st.session_state
    d = ss.driver

    per_step = int(ss.articles_per_step)
    processed = 0

    while ss.validate_i < len(ss.validate_ids) and processed < per_step and ss.running:
        aid = ss.validate_ids[ss.validate_i]
        base = ss.candidates.get(aid)
        ss.validate_i += 1
        processed += 1
        if not base:
            continue

        href = base["link"]
        ss.last_url = href

        dt = get_article_datetime_strict(d, href, pause=ss.pause)

        # ì‘ì„±ì¼ ëª» ì½ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ë²„ë¦¼ (ë‹¤ë¥¸ë‚ ì§œ ì„ì„ ë°©ì§€)
        if not dt:
            continue

        if dt.date() != ss.target_date:
            continue

        out = dict(base)
        out["date_detail"] = dt.strftime("%Y-%m-%d %H:%M")
        ss.collected[aid] = out

    if ss.validate_i >= len(ss.validate_ids):
        finalize_job()


# =========================
# ì¤‘ë³µ/ìœ ì‚¬
# =========================
def compute_author_dups(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=["date", "author", "count"])
    a = df.groupby(["date", "author"]).size().reset_index(name="count")
    return a[a["count"] >= 2].sort_values(by="count", ascending=False)


def compute_exact_dups(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=df.columns)
    return df[df.duplicated(subset=["date", "title_norm"], keep=False)].copy()


def compute_keyword_groups(df: pd.DataFrame, min_count: int = 2):
    if df.empty:
        return pd.DataFrame(columns=["keyword", "count", "examples"])

    tokens_list = [tokenize(t) for t in df["title"].fillna("").astype(str).tolist()]
    inv = {}
    for idx, toks in enumerate(tokens_list):
        for tok in set(toks):
            inv.setdefault(tok, []).append(idx)

    rows = []
    for kw, idxs in inv.items():
        if len(idxs) >= min_count:
            ex = [df.iloc[i]["title"] for i in idxs[:3]]
            rows.append({"keyword": kw, "count": len(idxs), "examples": " | ".join(ex)})

    out = pd.DataFrame(rows)
    return out.sort_values(by=["count", "keyword"], ascending=[False, True]) if not out.empty else out


def compute_ai_similar(df: pd.DataFrame, threshold: float = 0.78) -> pd.DataFrame:
    cols = ["title_a", "title_b", "similarity", "link_a", "link_b"]
    if df.empty or len(df) < 2:
        return pd.DataFrame(columns=cols)

    titles_raw = df["title"].fillna("").astype(str).tolist()
    titles = df["title_norm"].fillna("").astype(str).tolist()
    links = df["link"].fillna("").astype(str).tolist()

    vec_w = TfidfVectorizer(analyzer="word", ngram_range=(1, 2), min_df=1)
    Xw = vec_w.fit_transform(titles)
    Mw = cosine_similarity(Xw)

    vec_c = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 5), min_df=1)
    Xc = vec_c.fit_transform(titles)
    Mc = cosine_similarity(Xc)

    M = 0.55 * Mw + 0.45 * Mc

    rows = []
    n = len(titles)
    for i in range(n):
        for j in range(i + 1, n):
            s = float(M[i, j])
            if s >= threshold:
                rows.append({
                    "title_a": titles_raw[i],
                    "title_b": titles_raw[j],
                    "similarity": round(s, 3),
                    "link_a": links[i],
                    "link_b": links[j],
                })

    out = pd.DataFrame(rows, columns=cols)
    return out.sort_values(by="similarity", ascending=False) if not out.empty else out


# =========================
# UI
# =========================
ensure_state()

with st.expander("ì„¤ì •", expanded=True):
    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
    with c1:
        target_date = st.date_input("ë‚ ì§œ ì„ íƒ(âœ… ì´ ë‚ ì§œë§Œ)", value=kst_today())
    with c2:
        headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤", value=True)
    with c3:
        # ë¬´ì•¼ ìƒí™©(20ê°œ ë¯¸ë§Œ/1í˜ì´ì§€) ê¸°ì¤€: í¬ê²Œ ëŒ í•„ìš” ì—†ìŒ
        max_pages = st.number_input("ìµœëŒ€ í˜ì´ì§€(ì¶”ì²œ 3~5)", min_value=1, max_value=30, value=5, step=1)
    with c4:
        pause = st.number_input("ëŒ€ê¸°(ì´ˆ)", min_value=0.00, max_value=1.00, value=0.08, step=0.01)

    c5, c6, c7 = st.columns([1, 1, 1])
    with c5:
        pages_per_step = st.number_input("í•œ ë²ˆì— ëª©ë¡ í˜ì´ì§€ ì²˜ë¦¬", min_value=1, max_value=5, value=1, step=1)
    with c6:
        articles_per_step = st.number_input("í•œ ë²ˆì— ìƒì„¸ ê²€ì¦", min_value=1, max_value=30, value=10, step=1)
    with c7:
        auto_run = st.checkbox("ìë™ ì§„í–‰(ì¼œë©´ ê³„ì†)", value=True)

    st.session_state.pages_per_step = int(pages_per_step)
    st.session_state.articles_per_step = int(articles_per_step)

st.divider()

b1, b2, b3, b4 = st.columns([1, 1, 1, 2])
with b1:
    if st.button("â–¶ ì‹œì‘", use_container_width=True):
        try:
            start_job(target_date, headless, int(max_pages), float(pause))
            st.rerun()
        except Exception:
            st.error("ì‹œì‘ ì˜¤ë¥˜")
            st.code(traceback.format_exc())

with b2:
    if st.button("â­ ì§„í–‰(í•œ ë²ˆ)", use_container_width=True):
        st.session_state.running = True
        st.rerun()

with b3:
    if st.button("â¹ ì¤‘ì§€", use_container_width=True):
        stop_job()
        st.rerun()

with b4:
    debug = st.checkbox("ğŸª² ë””ë²„ê·¸ ë³´ê¸°", value=False)

# ì§„í–‰ í‘œì‹œ
phase = st.session_state.phase
running = st.session_state.running

status = st.empty()
pbar1 = st.progress(0)
pbar2 = st.progress(0)

# ì§„í–‰ë¥ (ëŒ€ëµ)
if phase in ("collect", "validate", "done"):
    # ëª©ë¡
    maxp = max(1, int(st.session_state.get("max_pages", int(max_pages))))
    curp = min(maxp, max(1, int(st.session_state.page)))
    pbar1.progress(int(min(1.0, curp / maxp) * 100))
    # ìƒì„¸
    total = max(1, len(st.session_state.validate_ids))
    done = min(total, int(st.session_state.validate_i))
    pbar2.progress(int(min(1.0, done / total) * 100))

if phase == "idle":
    status.info("ëŒ€ê¸° ì¤‘. â–¶ ì‹œì‘ì„ ëˆŒëŸ¬ì¤˜.")
elif phase == "collect":
    status.info(
        f"ëª©ë¡ ìˆ˜ì§‘ ì¤‘â€¦ page={st.session_state.page} / í›„ë³´={len(st.session_state.candidates)} "
        f"(ë§ˆì§€ë§‰ URL: {st.session_state.last_url})"
    )
elif phase == "validate":
    status.info(
        f"ìƒì„¸ ì‘ì„±ì¼ ê²€ì¦ ì¤‘â€¦ {st.session_state.validate_i} / {len(st.session_state.validate_ids)} "
        f"(í†µê³¼={len(st.session_state.collected)})"
    )
elif phase == "done":
    status.success(f"ì™„ë£Œ! ì„ íƒí•œ ë‚ ì§œ ê¸€ë§Œ {len(st.session_state.posts)}ê°œ")

if debug:
    st.caption("DEBUG LOG (ìµœê·¼ 200ì¤„)")
    st.code("\n".join(st.session_state.logs[-200:]) if st.session_state.logs else "(ë¡œê·¸ ì—†ìŒ)")
    st.caption(f"last_url = {st.session_state.last_url}")

# ì‘ì—… ìŠ¤í… ì‹¤í–‰
if running and phase in ("collect", "validate"):
    try:
        if phase == "collect":
            step_collect()
        elif phase == "validate":
            step_validate()
    except Exception as e:
        log(f"ì¹˜ëª… ì˜¤ë¥˜: {type(e).__name__}: {e}")
        st.session_state.running = False

    if auto_run and st.session_state.running and st.session_state.phase in ("collect", "validate"):
        time.sleep(0.12)
        st.rerun()

# ê²°ê³¼ í‘œì‹œ
df = (
    pd.DataFrame(st.session_state.posts)
    if st.session_state.posts
    else pd.DataFrame(columns=["date", "date_raw", "date_detail", "author", "title", "title_norm", "link"])
)

keyword_min_count = st.number_input("í‚¤ì›Œë“œ ì¤‘ë³µ ìµœì†Œ ê±´ìˆ˜", min_value=2, max_value=20, value=2, step=1)
sim_threshold = st.slider("AI ìœ ì‚¬ë„ ê¸°ì¤€", 0.50, 0.99, 0.78, 0.01)

author_dups = compute_author_dups(df)
exact_dups = compute_exact_dups(df)
keyword_groups = compute_keyword_groups(df, min_count=int(keyword_min_count))
ai_similar = compute_ai_similar(df, threshold=float(sim_threshold))

tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Œ ì›ë³¸", "ğŸš¨ ì‘ì„±ì ë™ì¼", "ğŸ§¨ ì œëª© ë™ì¼", "ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", "ğŸ¤– AI ìœ ì‚¬"])

with tab1:
    st.dataframe(df, use_container_width=True)

with tab2:
    st.dataframe(author_dups if not author_dups.empty else pd.DataFrame(), use_container_width=True)

with tab3:
    st.dataframe(exact_dups if not exact_dups.empty else pd.DataFrame(), use_container_width=True)

with tab4:
    st.dataframe(keyword_groups if not keyword_groups.empty else pd.DataFrame(), use_container_width=True)

with tab5:
    st.dataframe(ai_similar if not ai_similar.empty else pd.DataFrame(), use_container_width=True)
