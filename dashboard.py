import re
import time
from datetime import datetime, date as date_cls, timedelta
from urllib.parse import urljoin

import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ---------------------
# KST ì²˜ë¦¬ (ë°°í¬ ì•ˆì „)
# ---------------------
try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


def kst_now() -> datetime:
    if KST:
        return datetime.now(KST)
    return datetime.utcnow() + timedelta(hours=9)


def kst_today() -> date_cls:
    return kst_now().date()


# =====================
# ê²Œì‹œíŒ ê³ ì •
# =====================
CLUB_ID = 28866679
MENU_ID = 178
BASE_URL = f"https://cafe.naver.com/f-e/cafes/{CLUB_ID}/menus/{MENU_ID}?viewType=L&page="

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Connection": "keep-alive",
}


# =====================
# ë‚ ì§œ í…ìŠ¤íŠ¸ í•´ì„ -> ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
# =====================
def infer_date_from_list_text(date_text: str) -> date_cls | None:
    """
    ë„¤ì´ë²„ ëª©ë¡ì— ë³´ì´ëŠ” ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ í•´ì„í•œë‹¤.
    - HH:MM  -> ì˜¤ëŠ˜(KST)
    - YYYY.MM.DD -> í•´ë‹¹ ë‚ ì§œ
    (ë°°í¬/ë¡œì»¬ í‘œê¸° ì°¨ì´ ëª¨ë‘ ì»¤ë²„)
    """
    s = (date_text or "").strip()

    # HH:MM ì´ë©´ ì˜¤ëŠ˜ë¡œ í•´ì„
    if re.match(r"^\d{1,2}:\d{2}$", s):
        return kst_today()

    # YYYY.MM.DD ì´ë©´ íŒŒì‹±
    m = re.match(r"^(\d{4})\.(\d{2})\.(\d{2})$", s)
    if m:
        y, mo, d = map(int, m.groups())
        try:
            return date_cls(y, mo, d)
        except Exception:
            return None

    return None


def is_target_date(date_text: str, target_date: date_cls) -> bool:
    inferred = infer_date_from_list_text(date_text)
    return inferred == target_date


# =====================
# í…ìŠ¤íŠ¸ ì •ê·œí™”
# =====================
def norm(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def normalize_title(s: str) -> str:
    s = norm(s).lower()
    s = re.sub(r"[^\wê°€-í£ ]+", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokens(s: str) -> list[str]:
    s = (s or "").lower()
    s = re.sub(r"[^0-9a-zê°€-í£ ]+", " ", s)
    parts = [p for p in s.split() if len(p) >= 2]
    return parts


# =====================
# ëª©ë¡ ìˆ˜ì§‘ (ê°•ê±´ íŒŒì‹± + ë””ë²„ê·¸)
# =====================
def fetch_list_page(page: int):
    url = BASE_URL + str(page)
    res = requests.get(url, headers=HEADERS, timeout=25, allow_redirects=True)
    return url, res


def collect_article_list(target_date: date_cls, max_pages: int = 30, debug: bool = False):
    articles = []

    debug_log = []
    for page in range(1, max_pages + 1):
        url, res = fetch_list_page(page)

        html = res.text or ""
        soup = BeautifulSoup(html, "html.parser")

        # âœ… 1) ê°€ì¥ ë¨¼ì € a.articleë¥¼ ì°¾ê³ 
        links = soup.select("a.article")

        # âœ… 2) ë°°í¬ì—ì„œ êµ¬ì¡°ê°€ ë‹¤ë¥´ë©´ /articles/ ë§í¬ë¥¼ ì¶”ê°€ë¡œ ì¡ëŠ”ë‹¤
        if not links:
            links = [a for a in soup.select("a[href]") if "/articles/" in (a.get("href") or "")]

        # ë””ë²„ê·¸: í˜ì´ì§€ë³„ ìƒíƒœ
        if debug:
            sample_dates = []
            for dt in soup.select("td.td_date")[:5]:
                sample_dates.append(dt.get_text(strip=True))
            debug_log.append({
                "page": page,
                "status": res.status_code,
                "final_url": res.url,
                "found_links": len(links),
                "sample_date_texts": ", ".join(sample_dates) if sample_dates else "(none)",
                "html_head": html[:300].replace("\n", " ")  # ë„ˆë¬´ ê¸¸ë©´ ì•ˆ ë³´ì—¬ì„œ ì•ë¶€ë¶„ë§Œ
            })

        # ë§Œì•½ ë§í¬ ìì²´ê°€ ê³„ì† 0ì´ë©´ â†’ ë” í˜ì´ì§€ ëŒë ¤ë„ ì˜ë¯¸ ì—†ìŒ(ëŒ€ê°œ ì°¨ë‹¨/ë¡œê·¸ì¸/ë‹¤ë¥¸ HTML)
        if page == 1 and not links:
            break

        for a in links:
            href = a.get("href") or ""
            if not href:
                continue

            # title
            title = a.get_text(strip=True)
            # ê°€ë” ë§í¬ í…ìŠ¤íŠ¸ê°€ ë¹ˆ ê²½ìš°ê°€ ìˆì–´ì„œ ìƒìœ„ ìš”ì†Œì—ì„œ ì°¾ì•„ë³´ê¸°
            if not title:
                title = a.get("title", "") or ""

            # dateëŠ” ë³´í†µ ê°™ì€ row(tr) ì•ˆ td.td_date
            date_text = ""
            row = a.find_parent("tr")
            if row:
                dt = row.select_one("td.td_date")
                if dt:
                    date_text = dt.get_text(strip=True)

                # ì‘ì„±ì(ìˆìœ¼ë©´)
                au = row.select_one("td.td_name")
                author = au.get_text(strip=True) if au else ""
            else:
                author = ""

            # ë‚ ì§œ í…ìŠ¤íŠ¸ê°€ ëª» ì¡í˜”ì„ ë•Œ: í˜ì´ì§€ ë‚´ ì²« td.td_dateë¥¼ â€œê·¼ì²˜â€ì—ì„œë¼ë„ ì‹œë„
            if not date_text:
                near = a.find_parent()
                if near:
                    dt2 = near.select_one("td.td_date") if hasattr(near, "select_one") else None
                    if dt2:
                        date_text = dt2.get_text(strip=True)

            # âœ… ë‚ ì§œ í•„í„°
            if not is_target_date(date_text, target_date):
                continue

            full_url = urljoin("https://cafe.naver.com", href)
            articles.append({
                "date": target_date.strftime("%Y-%m-%d"),
                "date_raw": date_text,
                "author": author,
                "title": title,
                "title_norm": normalize_title(title),
                "link": full_url,
            })

        time.sleep(0.25)

    return articles, debug_log


# =====================
# ë³¸ë¬¸ ìˆ˜ì§‘
# =====================
def fetch_content(url: str) -> str:
    try:
        res = requests.get(url, headers=HEADERS, timeout=25)
        if res.status_code != 200:
            return ""
        soup = BeautifulSoup(res.text, "html.parser")

        iframe = soup.select_one("iframe#cafe_main")
        if iframe and iframe.get("src"):
            iframe_url = urljoin("https://cafe.naver.com", iframe["src"])
            res2 = requests.get(iframe_url, headers=HEADERS, timeout=25)
            if res2.status_code != 200:
                return ""
            soup = BeautifulSoup(res2.text, "html.parser")

        content = soup.select_one("div.se-main-container")
        if not content:
            content = soup.select_one("div#postViewArea") or soup.select_one("div.ContentRenderer")
        if not content:
            return ""

        return content.get_text(" ", strip=True)
    except Exception:
        return ""


# =====================
# ì¤‘ë³µ íŒì •
# =====================
def dup_by_author(df: pd.DataFrame):
    groups = df[df["author"].astype(str).str.len() > 0].groupby("author").indices
    pairs = []
    for author, idxs in groups.items():
        if len(idxs) >= 2:
            idxs = list(idxs)
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    pairs.append((idxs[i], idxs[j], 1.0, f"ì‘ì„±ì ë™ì¼: {author}"))
    return pairs


def dup_by_title(df: pd.DataFrame):
    groups = df.groupby("title_norm").indices
    pairs = []
    for t, idxs in groups.items():
        if t and len(idxs) >= 2:
            idxs = list(idxs)
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    pairs.append((idxs[i], idxs[j], 1.0, "ì œëª© ë™ì¼"))
    return pairs


def dup_by_keywords(df: pd.DataFrame, jaccard_threshold: float = 0.6):
    token_sets = []
    for _, r in df.iterrows():
        token_sets.append(set(simple_tokens(f"{r.get('title','')} {r.get('content','')}")))

    pairs = []
    n = len(token_sets)
    for i in range(n):
        for j in range(i + 1, n):
            a, b = token_sets[i], token_sets[j]
            if not a or not b:
                continue
            score = len(a & b) / len(a | b) if (a | b) else 0.0
            if score >= jaccard_threshold:
                pairs.append((i, j, score, f"í‚¤ì›Œë“œ ì¤‘ë³µ(Jaccard {score:.2f})"))
    return pairs


def dup_by_ai(df: pd.DataFrame, threshold: float = 0.7):
    texts = df["content"].fillna("").astype(str).tolist()
    try:
        vectorizer = TfidfVectorizer(min_df=2)
        tfidf = vectorizer.fit_transform(texts)
    except Exception:
        vectorizer = TfidfVectorizer(min_df=1)
        tfidf = vectorizer.fit_transform(texts)

    sim = cosine_similarity(tfidf)
    pairs = []
    for i in range(len(sim)):
        for j in range(i + 1, len(sim)):
            if sim[i, j] >= threshold:
                pairs.append((i, j, float(sim[i, j]), f"AI ìœ ì‚¬(cos {sim[i,j]:.2f})"))
    return pairs


def build_pairs_table(df: pd.DataFrame, pairs: list[tuple]):
    rows = []
    for i, j, score, reason in pairs:
        rows.append({
            "A_idx": i,
            "A_title": df.loc[i, "title"],
            "A_author": df.loc[i, "author"],
            "A_link": df.loc[i, "link"],
            "B_idx": j,
            "B_title": df.loc[j, "title"],
            "B_author": df.loc[j, "author"],
            "B_link": df.loc[j, "link"],
            "score": round(float(score), 3),
            "reason": reason,
        })
    return pd.DataFrame(rows)


# =====================
# Streamlit UI
# =====================
st.set_page_config(page_title="í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µê²€ì‚¬", layout="wide")
st.markdown("<style>.block-container{max-width:1400px;}</style>", unsafe_allow_html=True)

st.title("ğŸ“Œ í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µê²€ì‚¬")

# ìƒë‹¨ í† ê¸€
colA, colB, colC, colD, colE = st.columns([1, 1, 1, 1, 1])
with colA:
    opt_original = st.toggle("ğŸ“Œ ì›ë³¸", value=True)
with colB:
    opt_author = st.toggle("ğŸš¨ ì‘ì„±ì ë™ì¼", value=True)
with colC:
    opt_title = st.toggle("ğŸ§· ì œëª© ë™ì¼", value=True)
with colD:
    opt_keyword = st.toggle("ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", value=False)
with colE:
    opt_ai = st.toggle("ğŸ¤– AI ìœ ì‚¬", value=True)

st.divider()

left, right = st.columns([1, 1])
with left:
    target_date = st.date_input("ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ ì„ íƒ (KST ê¸°ì¤€)", kst_today())
with right:
    max_pages = st.number_input("ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜", 1, 200, 10, 1)

with st.expander("âš™ï¸ ì¤‘ë³µ íŒì • ì˜µì…˜", expanded=False):
    ai_threshold = st.slider("ğŸ¤– AI ìœ ì‚¬ ì„ê³„ì¹˜ (cosine)", 0.1, 0.99, 0.70, 0.01)
    kw_threshold = st.slider("ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ ì„ê³„ì¹˜ (Jaccard)", 0.1, 0.99, 0.60, 0.01)

with st.expander("ğŸ§ª ë””ë²„ê·¸ (ë°°í¬ì—ì„œ ëª©ë¡ì´ 0ê°œë©´ ê¼­ ì—´ì–´ë´)", expanded=False):
    debug_mode = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ ì¼œê¸°(í˜ì´ì§€1 HTML/ìƒíƒœ í‘œì‹œ)", value=False)

st.divider()

if "df" not in st.session_state:
    st.session_state["df"] = None

run = st.button("ğŸ“¥ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘", type="primary")

if run:
    with st.spinner("ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘..."):
        articles, debug_log = collect_article_list(target_date, int(max_pages), debug=debug_mode)

    if debug_mode:
        st.subheader("ğŸ§ª ë””ë²„ê·¸ ë¡œê·¸")
        st.dataframe(pd.DataFrame(debug_log), use_container_width=True)

    if not articles:
        st.error("ëª©ë¡ì—ì„œ í•´ë‹¹ ë‚ ì§œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆì–´. (ë°°í¬ì—ì„œ ëª©ë¡ HTMLì´ ë‹¤ë¥´ê²Œ ë‚´ë ¤ì˜¤ëŠ”ì§€ ë””ë²„ê·¸ë¥¼ í™•ì¸í•´ì¤˜)")
        st.stop()

    st.success(f"ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")

    progress = st.progress(0.0)
    contents = []
    for i, art in enumerate(articles):
        contents.append(fetch_content(art["link"]))
        progress.progress((i + 1) / len(articles))
        time.sleep(0.15)

    df = pd.DataFrame(articles)
    df["content"] = contents
    st.session_state["df"] = df

df = st.session_state.get("df")
if df is not None:
    st.subheader("âœ… ìˆ˜ì§‘ ê²°ê³¼")

    if opt_original:
        st.dataframe(df[["date", "date_raw", "author", "title", "title_norm", "link"]], use_container_width=True)

    all_pairs = []
    if opt_author:
        all_pairs += dup_by_author(df)
    if opt_title:
        all_pairs += dup_by_title(df)
    if opt_keyword:
        all_pairs += dup_by_keywords(df, float(kw_threshold))
    if opt_ai:
        all_pairs += dup_by_ai(df, float(ai_threshold))

    if not (opt_author or opt_title or opt_keyword or opt_ai):
        st.info("ì¤‘ë³µ ê¸°ì¤€ ë²„íŠ¼ì„ í•˜ë‚˜ ì´ìƒ ì¼œì¤˜.")
        st.stop()

    if all_pairs:
        merged = {}
        for i, j, score, reason in all_pairs:
            key = (min(i, j), max(i, j))
            merged.setdefault(key, {"score": 0.0, "reasons": []})
            merged[key]["score"] = max(merged[key]["score"], float(score))
            merged[key]["reasons"].append(reason)

        final_pairs = [(i, j, v["score"], " / ".join(v["reasons"])) for (i, j), v in merged.items()]
        result_df = build_pairs_table(df, final_pairs).sort_values(["score"], ascending=False)

        st.subheader("âš ï¸ ì¤‘ë³µ ì˜ì‹¬ ê²°ê³¼")
        st.dataframe(result_df, use_container_width=True)
    else:
        st.success("ğŸ‰ ì„ íƒí•œ ê¸°ì¤€ì—ì„œëŠ” ì¤‘ë³µ ì˜ì‹¬ì´ ì—†ì–´!")
