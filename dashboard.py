import re
import time
from datetime import datetime, date as date_cls
from urllib.parse import urljoin

import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


# =====================
# ê²Œì‹œíŒ ê³ ì •
# =====================
CLUB_ID = 28866679
MENU_ID = 178
BASE_URL = f"https://cafe.naver.com/f-e/cafes/{CLUB_ID}/menus/{MENU_ID}?viewType=L&page="

HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept-Language": "ko-KR,ko;q=0.9",
}


# =====================
# ë‚ ì§œ íŒë³„ (ëª©ë¡ ê¸°ì¤€)
# =====================
def is_target_date(date_text: str, target_date: date_cls) -> bool:
    """
    KST ê¸°ì¤€:
    - ì„ íƒ ë‚ ì§œê°€ ì˜¤ëŠ˜ì´ë©´: 'HH:MM' í˜•íƒœë§Œ ìˆ˜ì§‘
    - ê·¸ ì™¸(ê³¼ê±° ë‚ ì§œ)ì´ë©´: 'YYYY.MM.DD' ì •í™•íˆ ì¼ì¹˜ë§Œ ìˆ˜ì§‘
    """
    today_kst = datetime.now(KST).date() if KST else datetime.now().date()

    if target_date == today_kst:
        return bool(re.match(r"^\d{1,2}:\d{2}$", date_text))

    return date_text == target_date.strftime("%Y.%m.%d")


# =====================
# í…ìŠ¤íŠ¸ ì •ê·œí™”
# =====================
def norm(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def normalize_title(s: str) -> str:
    s = norm(s).lower()
    s = re.sub(r"[^\wê°€-í£ ]+", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def simple_tokens(s: str) -> list[str]:
    """
    ì•„ì£¼ ë‹¨ìˆœ í† í°í™”(í•œ/ì˜/ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë¶„ë¦¬)
    - í‚¤ì›Œë“œ ì¤‘ë³µìš©
    """
    s = (s or "").lower()
    s = re.sub(r"[^0-9a-zê°€-í£ ]+", " ", s)
    parts = [p for p in s.split() if len(p) >= 2]
    return parts


# =====================
# ëª©ë¡ ìˆ˜ì§‘
# =====================
def collect_article_list(target_date: date_cls, max_pages: int = 30) -> list[dict]:
    articles = []
    target_str = target_date.strftime("%Y.%m.%d")
    for page in range(1, max_pages + 1):
        res = requests.get(BASE_URL + str(page), headers=HEADERS, timeout=20)
        if res.status_code != 200:
            break

        soup = BeautifulSoup(res.text, "html.parser")
        rows = soup.select("tr")

        stop_flag = False

        for row in rows:
            title_tag = row.select_one("a.article")
            date_tag = row.select_one("td.td_date")
            author_tag = row.select_one("td.td_name")  # ì‘ì„±ì(ëª©ë¡ì— ë³´ì´ëŠ” ê²½ìš°)

            if not title_tag or not date_tag:
                continue

            date_text = date_tag.get_text(strip=True)

            # âœ… ë‚ ì§œ í•„í„°ëŠ” "ëª©ë¡ì—ì„œë§Œ" ì ìš©
            if not is_target_date(date_text, target_date):
                # ê³¼ê±° ë‚ ì§œì˜ ê²½ìš°, ë” ì•„ë˜(ë” ì˜›ë‚ )ë¡œ ë‚´ë ¤ê°€ë©´ ì¤‘ë‹¨
                # date_textê°€ YYYY.MM.DDì¼ ë•Œë§Œ ë¹„êµ
                if re.match(r"^\d{4}\.\d{2}\.\d{2}$", date_text) and date_text < target_str:
                    stop_flag = True
                continue

            article_url = urljoin("https://cafe.naver.com", title_tag.get("href", ""))
            title = title_tag.get_text(strip=True)
            author = author_tag.get_text(strip=True) if author_tag else ""

            articles.append(
                {
                    "title": title,
                    "title_norm": normalize_title(title),
                    "author": author,
                    "url": article_url,
                    "date": date_text,
                }
            )

        if stop_flag:
            break

        time.sleep(0.25)

    return articles


# =====================
# ë³¸ë¬¸ ìˆ˜ì§‘
# =====================
def fetch_content(url: str) -> str:
    try:
        res = requests.get(url, headers=HEADERS, timeout=25)
        if res.status_code != 200:
            return ""
        soup = BeautifulSoup(res.text, "html.parser")

        iframe = soup.select_one("iframe#cafe_main")
        if iframe and iframe.get("src"):
            iframe_url = urljoin("https://cafe.naver.com", iframe["src"])
            res2 = requests.get(iframe_url, headers=HEADERS, timeout=25)
            if res2.status_code != 200:
                return ""
            soup = BeautifulSoup(res2.text, "html.parser")

        content = soup.select_one("div.se-main-container")
        if not content:
            # êµ¬í˜• ì—ë””í„° fallback
            content = soup.select_one("div#postViewArea") or soup.select_one("div.ContentRenderer")

        if not content:
            return ""

        return content.get_text(" ", strip=True)

    except Exception:
        return ""


# =====================
# ì¤‘ë³µ íŒì •ë“¤
# =====================
def dup_by_author(df: pd.DataFrame):
    # ê°™ì€ ì‘ì„±ì ê·¸ë£¹(2ê°œ ì´ìƒ)
    groups = df[df["author"].astype(str).str.len() > 0].groupby("author").indices
    pairs = []
    for author, idxs in groups.items():
        if len(idxs) >= 2:
            idxs = list(idxs)
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    pairs.append((idxs[i], idxs[j], 1.0, f"ì‘ì„±ì ë™ì¼: {author}"))
    return pairs


def dup_by_title(df: pd.DataFrame):
    groups = df.groupby("title_norm").indices
    pairs = []
    for t, idxs in groups.items():
        if t and len(idxs) >= 2:
            idxs = list(idxs)
            for i in range(len(idxs)):
                for j in range(i + 1, len(idxs)):
                    pairs.append((idxs[i], idxs[j], 1.0, "ì œëª© ë™ì¼"))
    return pairs


def dup_by_keywords(df: pd.DataFrame, jaccard_threshold: float = 0.6):
    """
    ì œëª©+ë³¸ë¬¸ í† í°ìœ¼ë¡œ Jaccard ìœ ì‚¬ë„
    """
    token_sets = []
    for _, r in df.iterrows():
        tokens = simple_tokens(f"{r.get('title','')} {r.get('content','')}")
        token_sets.append(set(tokens))

    pairs = []
    n = len(token_sets)
    for i in range(n):
        for j in range(i + 1, n):
            a, b = token_sets[i], token_sets[j]
            if not a or not b:
                continue
            inter = len(a & b)
            union = len(a | b)
            score = inter / union if union else 0.0
            if score >= jaccard_threshold:
                pairs.append((i, j, score, f"í‚¤ì›Œë“œ ì¤‘ë³µ(Jaccard {score:.2f})"))
    return pairs


def dup_by_ai(df: pd.DataFrame, threshold: float = 0.7):
    """
    TF-IDF cosine similarity
    """
    texts = df["content"].fillna("").astype(str).tolist()
    # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ê°€ ë§ìœ¼ë©´ min_df=1ë¡œ ì™„í™”
    try:
        vectorizer = TfidfVectorizer(min_df=2)
        tfidf = vectorizer.fit_transform(texts)
    except Exception:
        vectorizer = TfidfVectorizer(min_df=1)
        tfidf = vectorizer.fit_transform(texts)

    sim = cosine_similarity(tfidf)

    pairs = []
    for i in range(len(sim)):
        for j in range(i + 1, len(sim)):
            if sim[i, j] >= threshold:
                pairs.append((i, j, float(sim[i, j]), f"AI ìœ ì‚¬(cos {sim[i,j]:.2f})"))
    return pairs


def build_pairs_table(df: pd.DataFrame, pairs: list[tuple]):
    """
    pairs: (i, j, score, reason)
    """
    rows = []
    for i, j, score, reason in pairs:
        rows.append(
            {
                "A_idx": i,
                "A_title": df.loc[i, "title"],
                "A_author": df.loc[i, "author"],
                "A_url": df.loc[i, "url"],
                "B_idx": j,
                "B_title": df.loc[j, "title"],
                "B_author": df.loc[j, "author"],
                "B_url": df.loc[j, "url"],
                "score": round(float(score), 3),
                "reason": reason,
            }
        )
    return pd.DataFrame(rows)


# =====================
# Streamlit UI
# =====================
st.set_page_config(page_title="í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µê²€ì‚¬", layout="wide")

# âœ… í™”ë©´ í­/ë†’ì´ ëŠë‚Œì„ ë§ì¶”ëŠ” CSS (ê°•ì œëŠ” ì•„ë‹ˆê³  ìµœëŒ€í•œ ê·¼ì ‘)
st.markdown(
    """
<style>
.block-container {max-width: 1400px;}
</style>
""",
    unsafe_allow_html=True,
)

st.title("ğŸ“Œ í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µê²€ì‚¬")

# --- ìƒë‹¨ ë²„íŠ¼(í† ê¸€) ì˜ì—­ ---
colA, colB, colC, colD, colE = st.columns([1, 1, 1, 1, 1])
with colA:
    opt_original = st.toggle("ğŸ“Œ ì›ë³¸", value=True)
with colB:
    opt_author = st.toggle("ğŸš¨ ì‘ì„±ì ë™ì¼", value=True)
with colC:
    opt_title = st.toggle("ğŸ§· ì œëª© ë™ì¼", value=True)
with colD:
    opt_keyword = st.toggle("ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", value=False)
with colE:
    opt_ai = st.toggle("ğŸ¤– AI ìœ ì‚¬", value=True)

st.divider()

# --- ì„¤ì • ì˜ì—­ ---
left, right = st.columns([1, 1])

with left:
    target_date = st.date_input(
        "ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ ì„ íƒ (KST ê¸°ì¤€)",
        datetime.now(KST).date() if KST else datetime.now().date(),
    )

with right:
    max_pages = st.number_input("ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜", min_value=1, max_value=200, value=30, step=1)

# ê¸°ì¤€ë³„ ì„ê³„ì¹˜ ì˜µì…˜
with st.expander("âš™ï¸ ì¤‘ë³µ íŒì • ì˜µì…˜", expanded=False):
    ai_threshold = st.slider("ğŸ¤– AI ìœ ì‚¬ ì„ê³„ì¹˜ (cosine)", 0.1, 0.99, 0.70, 0.01)
    kw_threshold = st.slider("ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ ì„ê³„ì¹˜ (Jaccard)", 0.1, 0.99, 0.60, 0.01)

st.divider()

# ì„¸ì…˜ ì´ˆê¸°í™”
if "df" not in st.session_state:
    st.session_state["df"] = None

run = st.button("ğŸ“¥ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘", type="primary")

if run:
    # 1) ëª©ë¡ ìˆ˜ì§‘
    with st.spinner("ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘..."):
        articles = collect_article_list(target_date, max_pages=int(max_pages))

    if not articles:
        st.error("ëª©ë¡ì—ì„œ í•´ë‹¹ ë‚ ì§œ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆì–´. (ë‚ ì§œ/í˜ì´ì§€ ì„¤ì • í™•ì¸)")
        st.stop()

    st.success(f"ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")

    # 2) ë³¸ë¬¸ ìˆ˜ì§‘
    progress = st.progress(0.0)
    contents = []
    for i, art in enumerate(articles):
        contents.append(fetch_content(art["url"]))
        progress.progress((i + 1) / len(articles))
        time.sleep(0.15)

    df = pd.DataFrame(articles)
    df["content"] = contents

    st.session_state["df"] = df

# --- ê²°ê³¼ í‘œì‹œ ---
df = st.session_state.get("df")

if df is not None:
    st.subheader("âœ… ìˆ˜ì§‘ ê²°ê³¼")

    if opt_original:
        st.dataframe(
            df[["date", "author", "title", "url"]].copy(),
            use_container_width=True,
            hide_index=True,
        )

    # ì„ íƒëœ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µìŒ ë§Œë“¤ê¸°
    all_pairs = []
    if opt_author:
        all_pairs += dup_by_author(df)
    if opt_title:
        all_pairs += dup_by_title(df)
    if opt_keyword:
        all_pairs += dup_by_keywords(df, jaccard_threshold=float(kw_threshold))
    if opt_ai:
        all_pairs += dup_by_ai(df, threshold=float(ai_threshold))

    # ê¸°ì¤€ì´ í•˜ë‚˜ë„ ì„ íƒ ì•ˆ ëì„ ë•Œ
    if not (opt_author or opt_title or opt_keyword or opt_ai):
        st.info("ì¤‘ë³µ ê¸°ì¤€ ë²„íŠ¼ì„ í•˜ë‚˜ ì´ìƒ ì¼œì¤˜.")
        st.stop()

    # ê²°ê³¼ ì •ë¦¬(ê°™ì€ (i,j) ì¤‘ë³µ reason í•©ì¹˜ê¸°)
    if all_pairs:
        merged = {}
        for i, j, score, reason in all_pairs:
            key = (min(i, j), max(i, j))
            if key not in merged:
                merged[key] = {"score": score, "reasons": [reason]}
            else:
                merged[key]["score"] = max(merged[key]["score"], score)
                merged[key]["reasons"].append(reason)

        final_pairs = []
        for (i, j), v in merged.items():
            final_pairs.append((i, j, v["score"], " / ".join(v["reasons"])))

        result_df = build_pairs_table(df, final_pairs).sort_values(["score"], ascending=False)

        st.subheader("âš ï¸ ì¤‘ë³µ ì˜ì‹¬ ê²°ê³¼")
        st.dataframe(result_df, use_container_width=True, hide_index=True)

    else:
        st.success("ğŸ‰ ì„ íƒí•œ ê¸°ì¤€ì—ì„œëŠ” ì¤‘ë³µ ì˜ì‹¬ì´ ì—†ì–´!")
