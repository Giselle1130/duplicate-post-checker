import re
import time
import traceback
from datetime import datetime, date as date_cls
from urllib.parse import urljoin

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait


# =========================
# ê¸°ë³¸
# =========================
try:
    from zoneinfo import ZoneInfo
    KST = ZoneInfo("Asia/Seoul")
except Exception:
    KST = None


def kst_today() -> date_cls:
    return datetime.now(KST).date() if KST else datetime.now().date()


def clean(x: str) -> str:
    return (x or "").replace("\u200b", "").strip()


# =========================
# ëŒ€ìƒ ê²Œì‹œíŒ
# =========================
CLUB_ID = 28866679
MENU_ID = 178
BASE = "https://cafe.naver.com"

BASE_LIST_URL = (
    "https://cafe.naver.com/ArticleList.nhn"
    f"?search.clubid={CLUB_ID}"
    f"&search.menuid={MENU_ID}"
    "&search.boardtype=L"
)

LINK_CSS = "a[href*='articleid='], a[href*='/articles/']"
ARTICLEID_RE = re.compile(r"(?:[?&]articleid=(\d+))|(?:/articles/(\d+))")


def build_page_url(page: int) -> str:
    return f"{BASE_LIST_URL}&search.page={page}"


def extract_article_id(href: str) -> str:
    m = ARTICLEID_RE.search(href or "")
    if not m:
        return ""
    return m.group(1) or m.group(2) or ""


def switch_to_cafe_iframe(driver) -> bool:
    """cafe_main iframe ì•ˆì— ëª©ë¡ì´ ìˆëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ì•ˆì „í•˜ê²Œ ì „í™˜"""
    try:
        driver.switch_to.default_content()
    except Exception:
        pass

    # NAME ìš°ì„ 
    try:
        driver.switch_to.frame("cafe_main")
        return True
    except Exception:
        pass

    # ID ì‹œë„
    try:
        driver.switch_to.default_content()
        iframes = driver.find_elements(By.ID, "cafe_main")
        if iframes:
            driver.switch_to.frame("cafe_main")
            return True
    except Exception:
        pass

    return False


def wait_any_links(driver, timeout=12) -> bool:
    wait = WebDriverWait(driver, timeout)

    def has_links(d):
        return len(d.find_elements(By.CSS_SELECTOR, LINK_CSS)) > 0

    # iframe ì•ˆ
    if switch_to_cafe_iframe(driver):
        try:
            wait.until(has_links)
            return True
        except Exception:
            pass

    # iframe ë°–
    try:
        driver.switch_to.default_content()
    except Exception:
        pass

    try:
        wait.until(has_links)
        return True
    except Exception:
        return False


# =========================
# ëª©ë¡ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ/ì‹œê°„ ì¶”ì¶œ
# =========================
RE_TIME = re.compile(r"\b(\d{1,2}:\d{2})\b")
RE_DOTDATE = re.compile(r"\b(20\d{2}\.\d{2}\.\d{2})\.?\b")


def extract_time_token(text: str) -> str:
    m = RE_TIME.search(clean(text))
    return m.group(1) if m else ""


def extract_dot_date(text: str) -> str:
    m = RE_DOTDATE.search(clean(text))
    return m.group(1) if m else ""


# =========================
# ì œëª© ì •ê·œí™”/í† í°
# =========================
STOPWORDS = {
    "steam", "kakao", "paragon", "pubg",
    "í´ëœ", "í´ëœì›", "ëª¨ì§‘", "í™˜ì˜", "ê°€ì…",
    "ë””ìŠ¤ì½”ë“œ", "discord", "ì„œë²„",
    "ì´ˆë³´", "ì‹ ìƒ", "ì¹œëª©", "ê²½ìŸ", "ì§ì¥ì¸",
    "ì¼ë°˜", "ë­í¬", "ë­ê²œ", "ìŠ¤ì¿¼ë“œ", "ë“€ì˜¤", "ì†”ë¡œ",
    "ë‚´ì „", "ììœ ", "ì´ë²¤íŠ¸", "ì•ˆë‚´", "ê³µì§€",
}


def normalize_title(raw: str) -> str:
    t = clean(raw)
    t = re.sub(r"\s*\[\s*\d+\s*\]\s*$", "", t)
    t = re.sub(r"\s*\(\s*\d+\s*\)\s*$", "", t)
    t = re.sub(r"\[[^\]]{1,30}\]", " ", t)
    t = re.sub(r"\bLv\.?\s*\d+\b", " ", t, flags=re.IGNORECASE)
    t = re.sub(r"\b\d{1,2}\s*~\s*\d{1,2}\b", " ", t)
    t = re.sub(r"\b\d{1,2}\s*ì„¸\b", " ", t)
    t = re.sub(r"https?://\S+", " ", t)
    t = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", t)
    t = re.sub(r"\b\d+\b", " ", t)
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t


def tokenize(text: str):
    t = normalize_title(text)
    toks = re.findall(r"[a-z]+|[ê°€-í£]+", t)
    toks = [x for x in toks if len(x) >= 2]
    toks = [x for x in toks if x not in STOPWORDS]
    return toks


# =========================
# Selenium
# =========================
def make_driver(headless: bool = True) -> webdriver.Chrome:
    opts = Options()
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1400,900")
    opts.page_load_strategy = "eager"
    if headless:
        opts.add_argument("--headless=new")

    # Render/ë¦¬ëˆ…ìŠ¤ ì•ˆì •ì„±
    opts.add_argument("--remote-debugging-port=9222")

    # ì´ë¯¸ì§€ ì°¨ë‹¨
    opts.add_experimental_option("prefs", {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
    })

    # âœ… Selenium Manager ì‚¬ìš© (webdriver_manager ê¸ˆì§€)
    driver = webdriver.Chrome(options=opts)
    driver.implicitly_wait(0.2)
    return driver


def collect_from_list_only(
    target_date: date_cls,
    headless: bool,
    max_pages: int = 30,
    pause: float = 0.08,
    stop_if_no_new_pages: int = 2,
    status_cb=None,   # ì§„í–‰ ìˆ«ì ì¶œë ¥ìš© ì½œë°±
):
    """
    âœ… ëª©ë¡ì—ì„œë§Œ ìˆ˜ì§‘
    - ì„ íƒ ë‚ ì§œ í•„í„°ëŠ” ëª©ë¡ì— ì°íŒ ë‚ ì§œ/ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œë§Œ ì ìš©
    - í˜ì´ì§€ ì¤‘ë³µ/ì‹ ê·œ0 í˜ì´ì§€ê°€ ì—°ì†ì´ë©´ ì¡°ê¸° ì¢…ë£Œ
    """
    today = kst_today()
    is_today = (target_date == today)
    target_dot = target_date.strftime("%Y.%m.%d")
    target_iso = target_date.strftime("%Y-%m-%d")

    driver = make_driver(headless=headless)
    seen_ids = set()
    rows_out = []
    no_new_pages = 0

    try:
        for page in range(1, int(max_pages) + 1):
            url = build_page_url(page)

            if status_cb:
                status_cb(page, max_pages, len(seen_ids), url)

            driver.get(url)
            ok = wait_any_links(driver, timeout=12)
            time.sleep(pause)

            if not ok:
                # ë§í¬ê°€ ì—†ìœ¼ë©´ ë
                no_new_pages += 1
                if no_new_pages >= stop_if_no_new_pages:
                    break
                continue

            # iframe ì•ˆ/ë°– ì–´ëŠ ìª½ì´ë“  ë§í¬ ìˆ˜ì§‘
            # (wait_any_linksì—ì„œ iframe ì´ë™í–ˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ì—¬ê¸°ì„œë„ í˜„ì¬ ë¬¸ì„œì—ì„œ ë°”ë¡œ ì½ìŒ)
            links = driver.find_elements(By.CSS_SELECTOR, LINK_CSS)

            page_new = 0
            for a in links:
                try:
                    href = clean(a.get_attribute("href"))
                    if not href:
                        continue
                    if href.startswith("/"):
                        href = urljoin(BASE, href)

                    aid = extract_article_id(href)
                    if not aid or aid in seen_ids:
                        continue

                    # ì œëª©
                    title = clean(a.text) or clean(a.get_attribute("title"))
                    if not title:
                        continue

                    # í–‰ í…ìŠ¤íŠ¸(ì‘ì„±ì/ë‚ ì§œ/ì‹œê°„ì´ ì—¬ê¸°ì— ë“¤ì–´ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ)
                    row_text = ""
                    author = ""
                    date_raw = ""

                    try:
                        parent = a.find_element(By.XPATH, "./ancestor::*[self::tr or self::li][1]")
                        row_text = clean(parent.text)
                        # ë‚ ì§œ/ì‹œê°„ í† í°
                        hhmm = extract_time_token(row_text)
                        dot = extract_dot_date(row_text)

                        if is_today:
                            # ì˜¤ëŠ˜: "ì‹œê°„í‘œì‹œ"ê°€ ìˆëŠ” í–‰ë§Œ í†µê³¼
                            if not hhmm:
                                continue
                            # í˜¹ì‹œ ë‚ ì§œê°€ ê°™ì´ ì°íˆë©´, ê·¸ ë‚ ì§œê°€ targetì´ ì•„ë‹ˆë©´ ì œì™¸
                            if dot and dot != target_dot:
                                continue
                            date_raw = hhmm
                        else:
                            # ê³¼ê±°: ë‚ ì§œí‘œì‹œê°€ targetê³¼ ê°™ì€ ê²ƒë§Œ í†µê³¼
                            if not dot or dot != target_dot:
                                continue
                            # ê³¼ê±°ì¸ë° ì‹œê°„ë§Œ ì°í˜€ìˆìœ¼ë©´(ì¼ë¶€ UI) ë¶ˆí™•ì‹¤ â†’ ì œì™¸
                            if hhmm:
                                continue
                            date_raw = dot

                        # ì‘ì„±ì(ê°€ëŠ¥í•˜ë©´)
                        parts = [x.strip() for x in row_text.split("\n") if x.strip()]
                        parts = [p for p in parts if p != title]
                        bad = ["ì¡°íšŒ", "ì¢‹ì•„ìš”", "ëŒ“ê¸€", "ëŒ“ê¸€ìˆ˜", "ê³µì§€"]
                        parts = [p for p in parts if not any(b in p for b in bad)]
                        parts = [p for p in parts if not RE_TIME.fullmatch(p)]
                        parts = [p for p in parts if not RE_DOTDATE.fullmatch(p)]
                        author = parts[0] if parts else ""

                    except Exception:
                        # row_text ëª» ì½ìœ¼ë©´ í•„í„° ë¶ˆê°€ëŠ¥ â†’ ì œì™¸(ì„ì„ ë°©ì§€)
                        continue

                    seen_ids.add(aid)
                    page_new += 1

                    rows_out.append({
                        "date": target_iso,
                        "date_raw": date_raw,     # ëª©ë¡ì— ì°íŒ ê°’(ì‹œê°„/ë‚ ì§œ)
                        "author": author,
                        "title": title,
                        "title_norm": normalize_title(title),
                        "link": href,
                    })

                except Exception:
                    continue

            if page_new == 0:
                no_new_pages += 1
            else:
                no_new_pages = 0

            if no_new_pages >= stop_if_no_new_pages:
                break

            time.sleep(pause)

    finally:
        try:
            driver.quit()
        except Exception:
            pass

    df = pd.DataFrame(rows_out)
    if not df.empty:
        df = df.drop_duplicates(subset=["link"]).copy()
    return df


# =========================
# ì¤‘ë³µ/ìœ ì‚¬
# =========================
def compute_author_dups(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=["date", "author", "count"])
    a = df.groupby(["date", "author"]).size().reset_index(name="count")
    return a[a["count"] >= 2].sort_values(by="count", ascending=False)


def compute_exact_dups(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=df.columns)
    return df[df.duplicated(subset=["date", "title_norm"], keep=False)].copy()


def compute_keyword_groups(df: pd.DataFrame, min_count: int = 2):
    if df.empty:
        return pd.DataFrame(columns=["keyword", "count", "examples"])

    tokens_list = [tokenize(t) for t in df["title"].fillna("").astype(str).tolist()]
    inv = {}
    for idx, toks in enumerate(tokens_list):
        for tok in set(toks):
            inv.setdefault(tok, []).append(idx)

    rows = []
    for kw, idxs in inv.items():
        if len(idxs) >= min_count:
            ex = [df.iloc[i]["title"] for i in idxs[:3]]
            rows.append({"keyword": kw, "count": len(idxs), "examples": " | ".join(ex)})

    out = pd.DataFrame(rows)
    return out.sort_values(by=["count", "keyword"], ascending=[False, True]) if not out.empty else out


def compute_ai_similar(df: pd.DataFrame, threshold: float = 0.78) -> pd.DataFrame:
    cols = ["title_a", "title_b", "similarity", "link_a", "link_b"]
    if df.empty or len(df) < 2:
        return pd.DataFrame(columns=cols)

    titles_raw = df["title"].fillna("").astype(str).tolist()
    titles = df["title_norm"].fillna("").astype(str).tolist()
    links = df["link"].fillna("").astype(str).tolist()

    vec_w = TfidfVectorizer(analyzer="word", ngram_range=(1, 2), min_df=1)
    Xw = vec_w.fit_transform(titles)
    Mw = cosine_similarity(Xw)

    vec_c = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 5), min_df=1)
    Xc = vec_c.fit_transform(titles)
    Mc = cosine_similarity(Xc)

    M = 0.55 * Mw + 0.45 * Mc

    rows = []
    n = len(titles)
    for i in range(n):
        for j in range(i + 1, n):
            s = float(M[i, j])
            if s >= threshold:
                rows.append({
                    "title_a": titles_raw[i],
                    "title_b": titles_raw[j],
                    "similarity": round(s, 3),
                    "link_a": links[i],
                    "link_b": links[j],
                })

    out = pd.DataFrame(rows, columns=cols)
    return out.sort_values(by="similarity", ascending=False) if not out.empty else out


# =========================
# UI
# =========================
with st.expander("ì„¤ì •", expanded=True):
    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
    with c1:
        target_date = st.date_input("ë‚ ì§œ ì„ íƒ", value=kst_today())
    with c2:
        headless = st.checkbox("í—¤ë“œë¦¬ìŠ¤", value=True)
    with c3:
        max_pages = st.number_input("ìµœëŒ€ í˜ì´ì§€", min_value=1, max_value=300, value=30, step=5)
    with c4:
        pause = st.number_input("í˜ì´ì§€ ëŒ€ê¸°(ì´ˆ)", min_value=0.00, max_value=2.00, value=0.08, step=0.01)

st.divider()

# ì§„í–‰ í…ìŠ¤íŠ¸(ìˆ«ìë§Œ)
progress_box = st.empty()

def status_cb(page, max_pages, seen_cnt, url):
    # âœ… ìˆ«ì í…ìŠ¤íŠ¸ë§Œ
    progress_box.info(f"ì§„í–‰: {page}/{max_pages} pages | ìˆ˜ì§‘(ì¤‘ë³µì œì™¸): {seen_cnt} | URL: {url}")

if "posts" not in st.session_state:
    st.session_state.posts = []

if st.button("ìˆ˜ì§‘ ì‹œì‘", use_container_width=True):
    st.session_state.posts = []
    try:
        df = collect_from_list_only(
            target_date=target_date,
            headless=headless,
            max_pages=int(max_pages),
            pause=float(pause),
            stop_if_no_new_pages=2,
            status_cb=status_cb,
        )
        st.session_state.posts = df.to_dict("records")
        progress_box.success(f"ì™„ë£Œ: {len(df)}ê°œ (ëª©ë¡ ê¸°ì¤€)")
    except Exception:
        progress_box.error("ìˆ˜ì§‘ ì˜¤ë¥˜")
        st.code(traceback.format_exc())

df = pd.DataFrame(st.session_state.posts) if st.session_state.posts else pd.DataFrame(
    columns=["date", "date_raw", "author", "title", "title_norm", "link"]
)

# ë¶„ì„ ì˜µì…˜
keyword_min_count = st.number_input("í‚¤ì›Œë“œ ì¤‘ë³µ ìµœì†Œ ê±´ìˆ˜", min_value=2, max_value=20, value=2, step=1)
sim_threshold = st.slider("AI ìœ ì‚¬ë„ ê¸°ì¤€", 0.50, 0.99, 0.78, 0.01)

author_dups = compute_author_dups(df)
exact_dups = compute_exact_dups(df)
keyword_groups = compute_keyword_groups(df, min_count=int(keyword_min_count))
ai_similar = compute_ai_similar(df, threshold=float(sim_threshold))

tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Œ ì›ë³¸", "ğŸš¨ ì‘ì„±ì ë™ì¼", "ğŸ§¨ ì œëª© ë™ì¼", "ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", "ğŸ¤– AI ìœ ì‚¬"])
with tab1:
    st.dataframe(df, use_container_width=True)
with tab2:
    st.dataframe(author_dups if not author_dups.empty else pd.DataFrame(), use_container_width=True)
with tab3:
    st.dataframe(exact_dups if not exact_dups.empty else pd.DataFrame(), use_container_width=True)
with tab4:
    st.dataframe(keyword_groups if not keyword_groups.empty else pd.DataFrame(), use_container_width=True)
with tab5:
    st.dataframe(ai_similar if not ai_similar.empty else pd.DataFrame(), use_container_width=True)
