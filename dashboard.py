import re
import time
import traceback
from datetime import datetime, date as date_cls

import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# =========================
# ëŒ€ìƒ ê²Œì‹œíŒ ê³ ì •
# =========================
CLUB_ID = 28866679
MENU_ID = 178
BASE_LIST_URL = f"https://cafe.naver.com/f-e/cafes/{CLUB_ID}/menus/{MENU_ID}?viewType=L&page="

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
    "Referer": f"https://cafe.naver.com/f-e/cafes/{CLUB_ID}/menus/{MENU_ID}?viewType=L",
}


# =========================
# ìœ í‹¸
# =========================
def clean(x: str) -> str:
    return (x or "").replace("\u200b", "").strip()


def kst_today() -> date_cls:
    # Cloudì—ì„œë„ ì•ˆì „í•˜ê²Œ "ì„œë²„ ì‹œê°„ ê¸°ì¤€"ìœ¼ë¡œ date ì‚¬ìš©
    return datetime.now().date()


def extract_time_token(text: str) -> str:
    m = re.search(r"\b(\d{1,2}:\d{2})\b", clean(text))
    return m.group(1) if m else ""


def extract_date_token(text: str) -> str:
    # 2025.12.17 ë˜ëŠ” 2025.12.17.
    m = re.search(r"\b(20\d{2}\.\d{2}\.\d{2})\.?\b", clean(text))
    return m.group(1) if m else ""


def build_page_url(page: int) -> str:
    return BASE_LIST_URL + str(page)


def get_soup(page: int) -> BeautifulSoup:
    url = build_page_url(page)
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return BeautifulSoup(r.text, "lxml")


# =========================
# ì œëª© ì •ê·œí™”/í† í°
# =========================
STOPWORDS = {
    "steam", "kakao", "paragon", "pubg",
    "í´ëœ", "í´ëœì›", "ëª¨ì§‘", "í™˜ì˜", "ê°€ì…",
    "ë””ìŠ¤ì½”ë“œ", "discord", "ì„œë²„",
    "ì´ˆë³´", "ì‹ ìƒ", "ì¹œëª©", "ê²½ìŸ", "ì§ì¥ì¸",
    "ì¼ë°˜", "ë­í¬", "ë­ê²œ", "ìŠ¤ì¿¼ë“œ", "ë“€ì˜¤", "ì†”ë¡œ",
    "ë‚´ì „", "ììœ ", "ì´ë²¤íŠ¸", "ì•ˆë‚´", "ê³µì§€",
}

def normalize_title(raw: str) -> str:
    t = clean(raw)

    # ë ëŒ“ê¸€ìˆ˜ ì œê±°
    t = re.sub(r"\s*\[\s*\d+\s*\]\s*$", "", t)
    t = re.sub(r"\s*\(\s*\d+\s*\)\s*$", "", t)

    # [Steam] ê°™ì€ íƒœê·¸ ì œê±°
    t = re.sub(r"\[[^\]]{1,30}\]", " ", t)

    # LV / ë‚˜ì´/ë²”ìœ„ íŒ¨í„´ ì œê±°
    t = re.sub(r"\bLv\.?\s*\d+\b", " ", t, flags=re.IGNORECASE)
    t = re.sub(r"\b\d{1,2}\s*~\s*\d{1,2}\b", " ", t)
    t = re.sub(r"\b\d{1,2}\s*ì„¸\b", " ", t)

    # url ì œê±°
    t = re.sub(r"https?://\S+", " ", t)

    # ì´ëª¨ì§€/ê¸°í˜¸ ì œê±° (í•œ/ì˜/ìˆ«ì/ê³µë°±ë§Œ ìœ ì§€)
    t = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", t)

    # ìˆ«ì ë‹¨ë… ì œê±°
    t = re.sub(r"\b\d+\b", " ", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t


def tokenize(text: str):
    t = normalize_title(text)
    toks = re.findall(r"[a-z]+|[ê°€-í£]+", t)
    toks = [x for x in toks if len(x) >= 2]
    toks = [x for x in toks if x not in STOPWORDS]
    return toks


# =========================
# ëª©ë¡ ìˆ˜ì§‘ (Cloud OK / ë‚ ì§œ í•„í„°)
# =========================
def collect_posts_by_date(target_date: date_cls, max_pages: int, pause: float, strict_date: bool):
    """
    - ì˜¤ëŠ˜: ëª©ë¡ì— ì‹œê°„ì´ (HH:MM)ë¡œ ë‚˜ì˜¤ë¯€ë¡œ, ì‹œê°„ í† í°ì´ ìˆëŠ” ê²ƒë§Œ ëª¨ìœ¼ê¸°
    - ê³¼ê±°: ëª©ë¡ì— 2025.12.17. í˜•íƒœê°€ ë‚˜ì˜¤ë¯€ë¡œ, ë‚ ì§œ í† í°ì´ targetê³¼ ê°™ìœ¼ë©´ ëª¨ìœ¼ê¸°
    - strict_date ì²´í¬ ì‹œ:
        * ê³¼ê±° ë‚ ì§œëŠ” ì¼ì¹˜í•˜ëŠ” ê¸€ë§Œ ìˆ˜ì§‘
        * ì˜¤ëŠ˜ì€ HH:MM ìˆëŠ” ê¸€ë§Œ ìˆ˜ì§‘
      (ê¸°ë³¸ ON ì¶”ì²œ)
    """
    today = kst_today()
    is_today = (target_date == today)
    target_dot = target_date.strftime("%Y.%m.%d")
    target_iso = target_date.strftime("%Y-%m-%d")

    items = []

    for page in range(1, int(max_pages) + 1):
        soup = get_soup(page)

        # ëª©ë¡ì—ì„œ ê¸€ ë§í¬ í›„ë³´ ì°¾ê¸°
        anchors = soup.select("a[href*='/f-e/cafes/'][href*='/articles/'], a[href*='/articles/']")
        if not anchors:
            break

        for a in anchors:
            href = a.get("href") or ""
            title = clean(a.get_text(" ", strip=True))
            if not title:
                continue

            # article id ì¶”ì¶œ
            m = re.search(r"/articles/(\d+)", href)
            if not m:
                continue
            article_id = m.group(1)

            link = f"https://cafe.naver.com/f-e/cafes/{CLUB_ID}/articles/{article_id}?boardtype=L&menuid={MENU_ID}"

            # âœ… ë‚ ì§œ/ì‹œê°„ì€ ë§í¬ ì£¼ë³€ í…ìŠ¤íŠ¸ì—ì„œ ëŒ€ì¶© ê¸ê¸° (HTML êµ¬ì¡°ê°€ ìì£¼ ë°”ë€Œì–´ì„œ 'ëŠìŠ¨í•œ íŒŒì‹±')
            # ê°€ì¥ ì‹¤ìš©ì ì¸ ë°©ì‹: anchorì˜ ë¶€ëª¨ í…ìŠ¤íŠ¸ì—ì„œ í† í° ì°¾ê¸°
            context_text = ""
            try:
                context_text = clean(a.parent.get_text(" ", strip=True))
            except Exception:
                context_text = title

            hhmm = extract_time_token(context_text)
            dot = extract_date_token(context_text)

            if strict_date:
                if is_today:
                    # ì˜¤ëŠ˜ì€ HH:MM ìˆëŠ” ê¸€ë§Œ
                    if not hhmm:
                        continue
                    date_raw = hhmm
                else:
                    # ê³¼ê±°ëŠ” ë‚ ì§œ í† í°ì´ targetê³¼ ê°™ì•„ì•¼
                    if not dot or dot != target_dot:
                        continue
                    date_raw = dot
            else:
                # ëŠìŠ¨ ëª¨ë“œ: í† í°ì´ ìˆìœ¼ë©´ ë„£ê³ , ì—†ìœ¼ë©´ ë¹ˆ ê°’
                date_raw = hhmm or dot or ""

            items.append({
                "date": target_iso,
                "date_raw": date_raw,
                "author": "",  # Cloud HTMLë§Œìœ¼ë¡œëŠ” ì•ˆì •ì ìœ¼ë¡œ ëª» ë½‘ì•„ì„œ ë¹„ì›€
                "title": title,
                "title_norm": normalize_title(title),
                "link": link,
            })

        time.sleep(float(pause))

    df = pd.DataFrame(items)
    if not df.empty:
        df = df.drop_duplicates(subset=["link"]).copy()
        df = df.sort_values(by="date_raw", ascending=False)
    return df.to_dict("records")


# =========================
# ì¤‘ë³µ/ìœ ì‚¬
# =========================
def compute_keyword_groups(df: pd.DataFrame, min_count: int = 2):
    if df.empty:
        return pd.DataFrame(columns=["keyword", "count", "examples"])

    tokens_list = []
    for _, row in df.iterrows():
        toks = tokenize(row["title"])
        tokens_list.append(toks)

    inv = {}
    for idx, toks in enumerate(tokens_list):
        for tok in set(toks):
            inv.setdefault(tok, []).append(idx)

    rows = []
    for kw, idxs in inv.items():
        if len(idxs) >= min_count:
            ex = [df.iloc[i]["title"] for i in idxs[:3]]
            rows.append({"keyword": kw, "count": len(idxs), "examples": " | ".join(ex)})

    out = pd.DataFrame(rows)
    if out.empty:
        return out
    return out.sort_values(by=["count", "keyword"], ascending=[False, True])


def compute_ai_similar(df: pd.DataFrame, threshold: float = 0.78) -> pd.DataFrame:
    cols = ["title_a", "title_b", "similarity", "link_a", "link_b"]
    if df.empty or len(df) < 2:
        return pd.DataFrame(columns=cols)

    titles_raw = df["title"].fillna("").astype(str).tolist()
    titles = df["title_norm"].fillna("").astype(str).tolist()
    links = df["link"].fillna("").astype(str).tolist()

    vec_c = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 5), min_df=1)
    Xc = vec_c.fit_transform(titles)
    M = cosine_similarity(Xc)

    rows = []
    n = len(titles)
    for i in range(n):
        for j in range(i + 1, n):
            s = float(M[i, j])
            if s >= threshold:
                rows.append({
                    "title_a": titles_raw[i],
                    "title_b": titles_raw[j],
                    "similarity": round(s, 3),
                    "link_a": links[i],
                    "link_b": links[j],
                })

    out = pd.DataFrame(rows, columns=cols)
    return out.sort_values(by="similarity", ascending=False) if not out.empty else out


# =========================
# UI
# =========================
st.set_page_config(page_title="í´ëœ/ë°©ì†¡/ë””ìŠ¤ì½”ë“œ ì¤‘ë³µ ì²´í¬", layout="wide")
st.title("ğŸ° í´ëœ / ë°©ì†¡ / ë””ìŠ¤ì½”ë“œ ì¤‘ë³µ ì²´í¬ (Cloud ë²„ì „)")

with st.expander("ì„¤ì •", expanded=True):
    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
    with c1:
        target_date = st.date_input("ë‚ ì§œ ì„ íƒ", value=kst_today())
    with c2:
        max_pages = st.number_input("ìµœëŒ€ í˜ì´ì§€", min_value=1, max_value=200, value=30, step=5)
    with c3:
        pause = st.number_input("í˜ì´ì§€ ëŒ€ê¸°(ì´ˆ)", min_value=0.0, max_value=3.0, value=0.25, step=0.05)
    with c4:
        strict_date = st.checkbox("ë‚ ì§œ/ì‹œê°„ ì—„ê²© ë¹„êµ(ì¶”ì²œ)", value=True)

keyword_min_count = st.number_input("í‚¤ì›Œë“œ ì¤‘ë³µ ìµœì†Œ ê±´ìˆ˜", min_value=2, max_value=20, value=2, step=1)
sim_threshold = st.slider("AI ìœ ì‚¬ë„ ê¸°ì¤€", 0.50, 0.99, 0.78, 0.01)

st.divider()

if st.button("ìˆ˜ì§‘ ì‹œì‘", use_container_width=True):
    st.session_state.posts = []
    try:
        posts = collect_posts_by_date(
            target_date=target_date,
            max_pages=int(max_pages),
            pause=float(pause),
            strict_date=bool(strict_date),
        )
        st.session_state.posts = posts
        st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ")
    except Exception:
        st.error("ìˆ˜ì§‘ ì˜¤ë¥˜")
        st.code(traceback.format_exc())

df = pd.DataFrame(st.session_state.posts) if "posts" in st.session_state and st.session_state.posts else pd.DataFrame(
    columns=["date", "date_raw", "author", "title", "title_norm", "link"]
)

keyword_groups = compute_keyword_groups(df, min_count=int(keyword_min_count))
ai_similar = compute_ai_similar(df, threshold=float(sim_threshold))

tab1, tab2, tab3 = st.tabs(["ğŸ“Œ ì›ë³¸", "ğŸ” í‚¤ì›Œë“œ ì¤‘ë³µ", "ğŸ¤– AI ìœ ì‚¬"])

with tab1:
    st.dataframe(df, use_container_width=True)

with tab2:
    if keyword_groups.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")
    else:
        st.dataframe(keyword_groups, use_container_width=True)

with tab3:
    if ai_similar.empty:
        st.info("í•´ë‹¹ ì—†ìŒ")
    else:
        st.dataframe(ai_similar, use_container_width=True)
